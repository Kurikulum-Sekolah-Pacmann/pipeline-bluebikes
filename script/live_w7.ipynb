{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e77c36",
   "metadata": {},
   "source": [
    "# Week #7 - Live Class\n",
    "Data Pipeline Course - Sekolah Engineer - Pacmann Academy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44aa83",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfda11",
   "metadata": {},
   "source": [
    "Objective:\n",
    "1. Create Data Pipeline for integrating bluebikes data with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7be6c5",
   "metadata": {},
   "source": [
    "## Case Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0a4e9",
   "metadata": {},
   "source": [
    "1. `Problem` <br>\n",
    "Sebuah perusahaan bernama bluebike ingin menganalisa data subcriber yang pernah mengunakan jasa mereka. Data transaksi trip masih tersimpan dalam file csv untuk setiap tahunnya. Sedangkan data object seperti station dan bike terdapat di database.\n",
    "    - File CSV: data trip yang dilakukan untuk tiap tahunnya (2019 dan 2020)\n",
    "    - BlueBikes: data obeject station dan bike dari perusahaan bluebikes\n",
    "\n",
    "2. `Solution` <br>\n",
    "To address these issues, an ETL (Extract, Transform, Load) pipeline will be developed. This pipeline will extract data from the different sources, apply necessary transformations to clean and standardize the data, and then load it into a unified data warehouse. \n",
    "The pipeline will have 2 Layers, Staging and Warehouse and have log system<br>\n",
    "\n",
    "<img src='https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/bluebikes_pipeline.drawio.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef97357",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234245e",
   "metadata": {},
   "source": [
    "`Docker Compose` and `repository`:\n",
    "[link](https://github.com/Kurikulum-Sekolah-Pacmann/pipeline-bluebikes.git)\n",
    "\n",
    "`Source Dataset`: \n",
    "- bluebikes_tripdata_csv: [link](https://www.kaggle.com/datasets/jackdaoud/bluebikes-in-boston)\n",
    "\n",
    "`Target Storage`: \n",
    "\n",
    "\n",
    "`Tools and Technologies`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7055660",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c92358",
   "metadata": {},
   "source": [
    "1. `Profiling` <br>\n",
    "Profiling involves analyzing and understanding the structure, content, and quality of the data from multiple sources within the clinic\n",
    "\n",
    "2. `Building Data Pipeline EL Source to Staging` <br>\n",
    "\n",
    "3. `Building Data Pipeline ETL Staging to Warehouse` <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ff1bb",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92e369",
   "metadata": {},
   "source": [
    "In this task, you will conduct a profiling of each table or spreadsheet provided in the dataset. The profiling process involves the following steps:\n",
    "1. Check Jumlah Kolom dan Nama Kolom\n",
    "2. Check Jumlah data\n",
    "3. Check Data Types\n",
    "4. Check Percentage of Missing Values\n",
    "5. Check Percentage of Valid Date Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26c18f",
   "metadata": {},
   "source": [
    "The first step in the profiling process is to `extract the data` from the source systems. This involves gathering data from various tables or csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5456473",
   "metadata": {},
   "source": [
    "When implementing modular code in your ETL PySpark process, it's essential to pass the Spark session as a parameter to your functions.\n",
    "This ensures that you remain within the same Spark session while executing related functions, maintaining consistency across your ETL operations.\n",
    "\n",
    "For example, if your main file is `pipeline_staging.py`, and you import the `extract_database(spark_session)` function `from src.staging.extract`, you would pass the Spark session to `extract_database()` when calling it. This keeps the function `within the Spark session` created in your main file, ensuring that all operations are performed within the same session context.\n",
    "<br><img src='https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/sparksession.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8dbf41",
   "metadata": {},
   "source": [
    "#### Load Your .env file\n",
    "\n",
    "create `helper.py` to create your database connection (src/utils)\n",
    "\n",
    "```\n",
    "DB_HOST_SOURCE=\"localhost\"\n",
    "DB_USER_SOURCE=\"postgres\"\n",
    "DB_PASS_SOURCE=\"aku\"\n",
    "DB_PORT_SOURCE=\"5432\"\n",
    "\n",
    "DB_HOST_TARGET=\"localhost\"\n",
    "DB_USER_TARGET=\"postgres\"\n",
    "DB_PASS_TARGET=\"aku\"\n",
    "DB_PORT_TARGET=\"5432\"\n",
    "\n",
    "\n",
    "DB_NAME_BLUEBIKES=\"bluebikes\"\n",
    "DB_NAME_STG=\"staging\"\n",
    "DB_NAME_LOG=\"etl_log\"\n",
    "DB_NAME_WH=\"warehouse\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3f1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST_SOURCE = os.getenv(\"DB_HOST_SOURCE\")\n",
    "DB_USER_SOURCE = os.getenv(\"DB_USER_SOURCE\")\n",
    "DB_PASS_SOURCE = os.getenv(\"DB_PASS_SOURCE\")\n",
    "DB_PORT_SOURCE = os.getenv(\"DB_PORT_SOURCE\")\n",
    "\n",
    "DB_HOST_TARGET = os.getenv(\"DB_HOST_TARGET\")\n",
    "DB_USER_TARGET = os.getenv(\"DB_USER_TARGET\")\n",
    "DB_PASS_TARGET = os.getenv(\"DB_PASS_TARGET\")\n",
    "DB_PORT_TARGET = os.getenv(\"DB_PORT_TARGET\")\n",
    "\n",
    "DB_NAME_BLUEBIKES = os.getenv(\"DB_NAME_BLUEBIKES\")\n",
    "DB_NAME_STG = os.getenv(\"DB_NAME_STG\")\n",
    "DB_NAME_LOG = os.getenv(\"DB_NAME_LOG\")\n",
    "DB_NAME_WH = os.getenv(\"DB_NAME_WH\")\n",
    "\n",
    "# Create URL link for each database connection\n",
    "\n",
    "def bluebikes_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_SOURCE}:{DB_PORT_SOURCE}/{DB_NAME_BLUEBIKES}\"\n",
    "    return DB_URL, DB_USER_SOURCE, DB_PASS_SOURCE\n",
    "\n",
    "def stg_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_STG}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET\n",
    "\n",
    "def log_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_LOG}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET\n",
    "\n",
    "def wh_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_WH}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7d4d5",
   "metadata": {},
   "source": [
    "#### Extarct Data From Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041df67",
   "metadata": {},
   "source": [
    "Buat file src/profiling/extract.py untuk menyimpan fungsi extract_database dan extract_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c33ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import bluebikes_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_database(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = bluebikes_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    # read data\n",
    "    df = spark \\\n",
    "              .read \\\n",
    "              .jdbc(url = DB_URL,\n",
    "                    table = table_name,\n",
    "                    properties = connection_properties)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e792adf",
   "metadata": {},
   "source": [
    "#### Extarct Data From CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eeae7df-dc3c-43a9-8ad7-1a0ea61d0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_csv(spark: SparkSession, file_name):\n",
    "\n",
    "    # read data\n",
    "    df = spark.read.csv(PATH + file_name, header=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15213c61-3cfd-458e-9b36-8c193cc8a6d2",
   "metadata": {},
   "source": [
    "#### Profiling Data From Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a4b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b62de9b-d293-4d7b-8a43-19db64af208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Profiling Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95a068",
   "metadata": {},
   "source": [
    "Extarct Data Table bike, station and user_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0704692-7fdf-4bd8-862c-b354952a11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.profiling.extract import extract_database, extract_csv\n",
    "\n",
    "df_user_type = extract_database(spark, 'user_type')\n",
    "df_station = extract_database(spark, 'station')\n",
    "df_bike = extract_database(spark, 'bike')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a113819-0655-40dd-a1db-1cfe3ce12ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|user_type_id|user_type_name|\n",
      "+------------+--------------+\n",
      "|           0|    Subscriber|\n",
      "|           1|      Customer|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5ab31f-368b-4da1-b856-bcb72afbbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+\n",
      "|station_id|        station_name|          latitude|         longitude|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "|         1|18 Dorrance Wareh...|         42.387151|        -71.075978|\n",
      "|         3|Colleges of the F...| 42.34011512249237|-71.10061883926393|\n",
      "|         4|Tremont St at E B...|         42.345392|        -71.069616|\n",
      "|         5|Northeastern Univ...|         42.341814|        -71.090179|\n",
      "|         6|Cambridge St at J...|42.361211653079856|-71.06530619789737|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_station.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3eea7e9-ae22-4a79-8cac-a5b31e3ab2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|bike_id|    type| model|purchase_date|\n",
      "+-------+--------+------+-------------+\n",
      "|      1|    road|R88W4N|   2020-12-02|\n",
      "|      4|    road|G79YM9|   2022-10-08|\n",
      "|      7|foldable|IOEZRL|   2024-07-07|\n",
      "|      8|foldable|ADQ2DA|   2021-03-26|\n",
      "|      9|foldable|8EEQ8Z|   2023-03-04|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bike.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb0820",
   "metadata": {},
   "source": [
    "Extract Data CSV bluebikes_tripdata_2019.csv and bluebikes_tripdata_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23097ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_2020 = extract_csv(spark, 'bluebikes_tripdata_2020.csv')\n",
    "df_trip_2019 = extract_csv(spark, 'bluebikes_tripdata_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff68e592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|postal code|year|month|birth year|gender|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|        1793|2020-11-01 00:00:...|2020-11-01 00:30:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  4896|  Customer|      11214|2020|   11|      NULL|  NULL|\n",
      "|        1832|2020-11-01 00:00:...|2020-11-01 00:31:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  5630|  Customer|      11220|2020|   11|      NULL|  NULL|\n",
      "|         262|2020-11-01 00:01:...|2020-11-01 00:06:...|             186|Congress St at No...|               42.3481|              -71.03764|             7|            Fan Pier|  42.353390507052296|   -71.04457139968872|  5634|Subscriber|      02128|2020|   11|      NULL|  NULL|\n",
      "|         419|2020-11-01 00:04:...|2020-11-01 00:10:...|              74|Harvard Square at...|    42.373267999999996|             -71.118579|            76|Central Sq Post O...|  42.366426000000004|   -71.10549499999999|  6071|  Customer|      02139|2020|   11|      NULL|  NULL|\n",
      "|         275|2020-11-01 00:04:...|2020-11-01 00:08:...|              73|Harvard Square at...|             42.373231|             -71.120886|           104|Harvard Universit...|           42.380287|           -71.125107|  2712|Subscriber|      21015|2020|   11|      NULL|  NULL|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|year|month|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|         790|2019-12-01 00:01:...|2019-12-01 00:14:...|             370|Dartmouth St at N...|     42.35096144421219|     -71.07782810926437|            33|      Kenmore Square|           42.348706|           -71.097009|  5133|Subscriber|      1950|     1|2019|   12|\n",
      "|         166|2019-12-01 00:05:...|2019-12-01 00:08:...|              80|MIT Stata Center ...|      42.3621312344991|     -71.09115600585936|            67|MIT at Mass Ave /...|             42.3581|           -71.093198|  2653|Subscriber|      1993|     1|2019|   12|\n",
      "|         323|2019-12-01 00:08:...|2019-12-01 00:13:...|             381|Inman Square at S...|     42.37438408515815|      -71.1001574621514|           221|Verizon Innovatio...|   42.37250864997261|   -71.11305356025694|  4875|Subscriber|      1992|     1|2019|   12|\n",
      "|         709|2019-12-01 00:08:...|2019-12-01 00:20:...|             185|     Third at Binney|    42.365444861373994|      -71.0827714204788|           184|Sidney Research C...|   42.35775309465199|   -71.10393404960631|  2116|Subscriber|      1997|     1|2019|   12|\n",
      "|         332|2019-12-01 00:10:...|2019-12-01 00:15:...|             221|Verizon Innovatio...|     42.37250864997261|     -71.11305356025694|            89|Harvard Law Schoo...|           42.379011|           -71.119945|  6156|Subscriber|      1985|     1|2019|   12|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_2020.show(5)\n",
    "df_trip_2019.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2812930",
   "metadata": {},
   "source": [
    "##### 1. Check the column size and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4300bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Column Count and Column Name ---\n",
      "Column in data user_type: ['user_type_id', 'user_type_name'], Count: 2\n",
      "Column in data station: ['station_id', 'station_name', 'latitude', 'longitude'], Count: 4\n",
      "Column in data bike: ['bike_id', 'type', 'model', 'purchase_date'], Count: 4\n",
      "Column in data trip_2020: ['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', 'start station latitude', 'start station longitude', 'end station id', 'end station name', 'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'postal code', 'year', 'month', 'birth year', 'gender'],  Count: 18\n",
      "Column in data trip_2019: ['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', 'start station latitude', 'start station longitude', 'end station id', 'end station name', 'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender', 'year', 'month'], Count: 17\n",
      "--- Data Size ---\n",
      "Data size user_type: 2\n",
      "Data size station: 387\n",
      "Data size bike: 4989\n",
      "Data size trip_2020: 1999446\n",
      "Data size trip_2019: 2522771\n"
     ]
    }
   ],
   "source": [
    "# show column count and column name\n",
    "print(\"--- Column Count and Column Name ---\")\n",
    "print(f\"Column in data user_type: {df_user_type.columns}, Count: {len(df_user_type.columns)}\")\n",
    "print(f\"Column in data station: {df_station.columns}, Count: {len(df_station.columns)}\")\n",
    "print(f\"Column in data bike: {df_bike.columns}, Count: {len(df_bike.columns)}\")\n",
    "print(f\"Column in data trip_2020: {df_trip_2020.columns},  Count: {len(df_trip_2020.columns)}\")\n",
    "print(f\"Column in data trip_2019: {df_trip_2019.columns}, Count: {len(df_trip_2019.columns)}\")\n",
    "\n",
    "# show data size\n",
    "print(\"--- Data Size ---\")\n",
    "print(f\"Data size user_type: {df_user_type.count()}\")\n",
    "print(f\"Data size station: {df_station.count()}\")\n",
    "print(f\"Data size bike: {df_bike.count()}\")\n",
    "print(f\"Data size trip_2020: {df_trip_2020.count()}\")\n",
    "print(f\"Data size trip_2019: {df_trip_2019.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552e825",
   "metadata": {},
   "source": [
    "- We get information data trip 2020 dan 2019 memiliki jumlah kolom yang berbeda, data 2020 memiliki informasi data postal code, sedangkan data 2019 tidak punya.\n",
    "- Data user yang melakukan trip terbatas hanya info 'usertype', 'birth year', 'gender', pada database juga tidak terdapat infoormasi user\n",
    "\n",
    "Next: cari tau data type untuk setiap kolom "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b16051",
   "metadata": {},
   "source": [
    "##### 2. Check the data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae86c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Type For Each Column---\n",
      "Data type for each column in user_type: [('user_type_id', 'bigint'), ('user_type_name', 'string')]\n",
      "Data type for each column in station: [('station_id', 'bigint'), ('station_name', 'string'), ('latitude', 'double'), ('longitude', 'double')]\n",
      "Data type for each column in bike: [('bike_id', 'bigint'), ('type', 'string'), ('model', 'string'), ('purchase_date', 'date')]\n",
      "Data type for each column in trip_2020: [('tripduration', 'string'), ('starttime', 'string'), ('stoptime', 'string'), ('start station id', 'string'), ('start station name', 'string'), ('start station latitude', 'string'), ('start station longitude', 'string'), ('end station id', 'string'), ('end station name', 'string'), ('end station latitude', 'string'), ('end station longitude', 'string'), ('bikeid', 'string'), ('usertype', 'string'), ('postal code', 'string'), ('year', 'string'), ('month', 'string'), ('birth year', 'string'), ('gender', 'string')]\n",
      "Data type for each column in trip_2019: [('tripduration', 'string'), ('starttime', 'string'), ('stoptime', 'string'), ('start station id', 'string'), ('start station name', 'string'), ('start station latitude', 'string'), ('start station longitude', 'string'), ('end station id', 'string'), ('end station name', 'string'), ('end station latitude', 'string'), ('end station longitude', 'string'), ('bikeid', 'string'), ('usertype', 'string'), ('birth year', 'string'), ('gender', 'string'), ('year', 'string'), ('month', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# get data type of each column\n",
    "print(\"--- Data Type For Each Column---\")\n",
    "print(f\"Data type for each column in user_type: {df_user_type.dtypes}\")\n",
    "print(f\"Data type for each column in station: {df_station.dtypes}\")\n",
    "print(f\"Data type for each column in bike: {df_bike.dtypes}\")\n",
    "print(f\"Data type for each column in trip_2020: {df_trip_2020.dtypes}\")\n",
    "print(f\"Data type for each column in trip_2019: {df_trip_2019.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503633d",
   "metadata": {},
   "source": [
    "- We get info kalau data yang diekstrak dari csv semua bertipe string, padahal ada beberapa yang memiliki tipe numeric jika dilihat dari snipet datanya\n",
    "\n",
    "Next: Check Percentage of Missing Values for each column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71501f",
   "metadata": {},
   "source": [
    "##### 3. Check Percentage of Missing Values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c5822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Percentage of Missing Values for each column with pyspark\n",
    "# output column_a : 0.0, column_b : 0.0, column_c : 0.0\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "def missing_value(df):\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    # use when function to check if the value is null then 1 otherwise None\n",
    "    missing = df.select([\n",
    "        (count(when(col(c).isNull(), c)) / total_count * 100).alias(c) \n",
    "        for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5a953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Value ---\n",
      "Data user_type\n",
      "{'user_type_id': 0.0, 'user_type_name': 0.0}\n",
      "Data station\n",
      "{'station_id': 0.0, 'station_name': 0.0, 'latitude': 0.0, 'longitude': 0.0}\n",
      "Data bike\n",
      "{'bike_id': 0.0, 'type': 0.0, 'model': 0.0, 'purchase_date': 0.0}\n",
      "Data trip_2020\n",
      "{'tripduration': 0.0, 'starttime': 0.0, 'stoptime': 0.0, 'start station id': 0.0, 'start station name': 0.0, 'start station latitude': 0.0, 'start station longitude': 0.0, 'end station id': 0.0, 'end station name': 0.0, 'end station latitude': 0.0, 'end station longitude': 0.0, 'bikeid': 0.0, 'usertype': 0.0, 'postal code': 27.873170868330526, 'year': 0.0, 'month': 0.0, 'birth year': 79.1954371360867, 'gender': 79.1954371360867}\n",
      "Data trip_2019\n",
      "{'tripduration': 0.0, 'starttime': 0.0, 'stoptime': 0.0, 'start station id': 0.0, 'start station name': 0.0, 'start station latitude': 0.0, 'start station longitude': 0.0, 'end station id': 0.0, 'end station name': 0.0, 'end station latitude': 0.0, 'end station longitude': 0.0, 'bikeid': 0.0, 'usertype': 0.0, 'birth year': 0.0, 'gender': 0.0, 'year': 0.0, 'month': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Missing Value ---\")\n",
    "print(\"Data user_type\")\n",
    "print(missing_value(df_user_type))\n",
    "print(\"Data station\")\n",
    "print(missing_value(df_station))\n",
    "print(\"Data bike\")\n",
    "print(missing_value(df_bike))\n",
    "print(\"Data trip_2020\")\n",
    "print(missing_value(df_trip_2020))\n",
    "print(\"Data trip_2019\")\n",
    "print(missing_value(df_trip_2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eece6",
   "metadata": {},
   "source": [
    "Informasi yang memiliki missing value adalah dari data trip 2020 koloom postal_code, birth_year dan gender\n",
    "Informasi user yang dibutuhkan nanti hanya user_type, jadi untuk kolom yang berhungungan dengan user selain user_type akan dihapus\n",
    "\n",
    "Next check Check Percentage of Valid Date Format\n",
    "Bike: column purchase_date \n",
    "check Check Percentage of Valid Datetime Format\n",
    "Data Trip 2020 & 2019: starttime, stoptime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f064c",
   "metadata": {},
   "source": [
    "##### 4. Check Percentage of Valid Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a3f3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Check Percentage of Valid Date Format\n",
    "# Table Bike: column purchase_date \n",
    "# check Check Percentage of Valid Datetime Format\n",
    "# Table Data Trip 2020 & 2019: starttime, stoptime\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "def valid_date_format(df, column_name):\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Calculate the percentage of valid date format for each column\n",
    "    # if data can be converted to date then it is valid\n",
    "    valid_date = df.select([\n",
    "        (count(when(col(column_name).isNotNull() & to_date(col(column_name), 'yyyy-MM-dd').isNotNull(), column_name)) / total_count  * 100).alias(column_name) \n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    return valid_date\n",
    "\n",
    "def valid_datetime_format(df, column_name):\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Calculate the percentage of valid datetime format for each column\n",
    "    # if data can be converted to datetime then it is valid\n",
    "    valid_datetime = df.select([\n",
    "        (count(when(col(column_name).isNotNull() & to_timestamp(col(column_name), 'yyyy-MM-dd HH:mm:ss.SSSS').isNotNull(), column_name)) / total_count  * 100).alias(column_name) \n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    return valid_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1135d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Valid Date Format ---\n",
      "Data bike\n",
      "{'purchase_date': 100.0}\n",
      "--- Valid Datetime Format ---\n",
      "Data trip_2019\n",
      "{'starttime': 100.0}\n",
      "{'stoptime': 100.0}\n",
      "Data trip_2020\n",
      "{'starttime': 100.0}\n",
      "{'stoptime': 100.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Valid Date Format ---\")\n",
    "print(\"Data bike\")\n",
    "print(valid_date_format(df_bike, \"purchase_date\"))\n",
    "print(\"--- Valid Datetime Format ---\")\n",
    "print(\"Data trip_2019\")\n",
    "print(valid_datetime_format(df_trip_2019, \"starttime\"))\n",
    "print(valid_datetime_format(df_trip_2019, \"stoptime\"))\n",
    "print(\"Data trip_2020\")\n",
    "print(valid_datetime_format(df_trip_2020, \"starttime\"))\n",
    "print(valid_datetime_format(df_trip_2020, \"stoptime\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1e6c0",
   "metadata": {},
   "source": [
    "Info yang didapatkan adalah semua format tanggal dan datetime valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5294f1b-4973-4c01-9502-cb959198a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Profiling Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bb696",
   "metadata": {},
   "source": [
    "### Building Data Pipeline EL Source to Staging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13298ebb",
   "metadata": {},
   "source": [
    "Solution: \n",
    "1. Pattern: EL\n",
    "    - `Data Extraction` involves retrieving data from various sources\n",
    "    - `Data Loading` involves transferring this raw data into staging systems\n",
    "2. Data Extraction:\n",
    "    - Sources: Extract data from csv file and database bluevikes.\n",
    "    - Techniques: Use Full Incremental for each source. Karena kedua source tidak memiliki penanda data baru (tidak memiliki kolom created at atau updatedat)\n",
    "3. Data Load:\n",
    "    - Staging: Load raw data into a staging database (PostgreSQL) without transformation.\n",
    "    - Techniques: overwrite, data di staging akan di overwrite setiap pipeline dijalankan\n",
    "Untuk setiap proses akan disimpan dalam log process\n",
    "4. Data Staging Schema:\n",
    "\n",
    "<img src='https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/w7_staging_-_public.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09283dc-6758-483d-b791-963b715143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline Staging\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125aed62",
   "metadata": {},
   "source": [
    "#### Log System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131f04f",
   "metadata": {},
   "source": [
    "Setiap proses akan memiliki informasi log infonya. pesan log berupa:\n",
    "\n",
    "```\n",
    "log_msg = {\n",
    "                \"step\" : \"staging | warehouse\",\n",
    "                \"process\":\"extraction | transformation | load\",\n",
    "                \"status\": \"success | failed\",\n",
    "                \"source\": \"db_bluebikes | csv | staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": Current timestamp\n",
    "            }\n",
    "```\n",
    "\n",
    "Pada utils.py tambahakan function untuk menyimpan pesan log ke database log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc19adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import stg_engine, log_engine, wh_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def load_log(spark: SparkSession, log_msg):\n",
    "    DB_URL, DB_USER, DB_PASS = log_engine()\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fa392da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of log message\n",
    "from datetime import datetime\n",
    "from src.utils.helper import load_log\n",
    "\n",
    "current_timestamp = datetime.now()\n",
    "\n",
    "log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"db_bluebikes\", \"user_type\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854f3fa8-6aac-4c6d-b796-528010736976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load log\n",
    "load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e569237",
   "metadata": {},
   "source": [
    "#### Extarct Data From Database bluebikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fcd97",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Buat modul extarct_database pada folder src/staging/extarct\n",
    "2. Get Database connection URL from database bluebikes\n",
    "3. Extarct data from specific table \n",
    "4. Save Log Info to database log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec429c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extarct with log\n",
    "from src.utils.helper import load_log, bluebikes_engine\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_database(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = bluebikes_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "    \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"db_bluebikes\", table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"failed\", \"db_bluebikes\", table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9faf94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.staging.extract.extract_database import extract_database\n",
    "\n",
    "# Extract data from database\n",
    "df_user_type = extract_database(spark, 'user_type')\n",
    "df_station = extract_database(spark, 'station')\n",
    "df_bike = extract_database(spark, 'bike')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cf23184-6c0d-4b00-b47d-7f37e0d60f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|user_type_id|user_type_name|\n",
      "+------------+--------------+\n",
      "|           0|    Subscriber|\n",
      "|           1|      Customer|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4c76927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+\n",
      "|station_id|        station_name|          latitude|         longitude|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "|         1|18 Dorrance Wareh...|         42.387151|        -71.075978|\n",
      "|         3|Colleges of the F...| 42.34011512249237|-71.10061883926393|\n",
      "|         4|Tremont St at E B...|         42.345392|        -71.069616|\n",
      "|         5|Northeastern Univ...|         42.341814|        -71.090179|\n",
      "|         6|Cambridge St at J...|42.361211653079856|-71.06530619789737|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_station.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a0f2847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|bike_id|    type| model|purchase_date|\n",
      "+-------+--------+------+-------------+\n",
      "|      1|    road|R88W4N|   2020-12-02|\n",
      "|      4|    road|G79YM9|   2022-10-08|\n",
      "|      7|foldable|IOEZRL|   2024-07-07|\n",
      "|      8|foldable|ADQ2DA|   2021-03-26|\n",
      "|      9|foldable|8EEQ8Z|   2023-03-04|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bike.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0853ce",
   "metadata": {},
   "source": [
    "#### Extarct Data From CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d2376",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Buat modul extarct_csv pada folder src/staging/extarct\n",
    "3. Extarct data from specific file\n",
    "4. Save Log Info to database log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c07fe86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "PATH = \"data/\"\n",
    "\n",
    "def extract_csv(spark: SparkSession, file_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = spark.read.csv(PATH + file_name, header=True)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"csv\", file_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"failed\", \"csv\", file_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e66dc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.staging.extract.extract_csv import extract_csv\n",
    "\n",
    "# Extract data from csv\n",
    "df_trip_2020 = extract_csv(spark, 'bluebikes_tripdata_2020.csv')\n",
    "df_trip_2019 = extract_csv(spark, 'bluebikes_tripdata_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cdb97f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|year|month|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|         790|2019-12-01 00:01:...|2019-12-01 00:14:...|             370|Dartmouth St at N...|     42.35096144421219|     -71.07782810926437|            33|      Kenmore Square|           42.348706|           -71.097009|  5133|Subscriber|      1950|     1|2019|   12|\n",
      "|         166|2019-12-01 00:05:...|2019-12-01 00:08:...|              80|MIT Stata Center ...|      42.3621312344991|     -71.09115600585936|            67|MIT at Mass Ave /...|             42.3581|           -71.093198|  2653|Subscriber|      1993|     1|2019|   12|\n",
      "|         323|2019-12-01 00:08:...|2019-12-01 00:13:...|             381|Inman Square at S...|     42.37438408515815|      -71.1001574621514|           221|Verizon Innovatio...|   42.37250864997261|   -71.11305356025694|  4875|Subscriber|      1992|     1|2019|   12|\n",
      "|         709|2019-12-01 00:08:...|2019-12-01 00:20:...|             185|     Third at Binney|    42.365444861373994|      -71.0827714204788|           184|Sidney Research C...|   42.35775309465199|   -71.10393404960631|  2116|Subscriber|      1997|     1|2019|   12|\n",
      "|         332|2019-12-01 00:10:...|2019-12-01 00:15:...|             221|Verizon Innovatio...|     42.37250864997261|     -71.11305356025694|            89|Harvard Law Schoo...|           42.379011|           -71.119945|  6156|Subscriber|      1985|     1|2019|   12|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_2019.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6ab9479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|postal code|year|month|birth year|gender|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|        1793|2020-11-01 00:00:...|2020-11-01 00:30:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  4896|  Customer|      11214|2020|   11|      NULL|  NULL|\n",
      "|        1832|2020-11-01 00:00:...|2020-11-01 00:31:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  5630|  Customer|      11220|2020|   11|      NULL|  NULL|\n",
      "|         262|2020-11-01 00:01:...|2020-11-01 00:06:...|             186|Congress St at No...|               42.3481|              -71.03764|             7|            Fan Pier|  42.353390507052296|   -71.04457139968872|  5634|Subscriber|      02128|2020|   11|      NULL|  NULL|\n",
      "|         419|2020-11-01 00:04:...|2020-11-01 00:10:...|              74|Harvard Square at...|    42.373267999999996|             -71.118579|            76|Central Sq Post O...|  42.366426000000004|   -71.10549499999999|  6071|  Customer|      02139|2020|   11|      NULL|  NULL|\n",
      "|         275|2020-11-01 00:04:...|2020-11-01 00:08:...|              73|Harvard Square at...|             42.373231|             -71.120886|           104|Harvard Universit...|           42.380287|           -71.125107|  2712|Subscriber|      21015|2020|   11|      NULL|  NULL|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_2020.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2d806",
   "metadata": {},
   "source": [
    "#### Load Data to Satging Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a06f7c",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Buat modul load_staging pada folder src/staging/load\n",
    "3. Load data to specific table\n",
    "4. Save Log Info to database log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b7dc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_log, stg_engine  \n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def load_staging(spark: SparkSession, df, table_name, source_name):\n",
    "    current_timestamp = datetime.now()\n",
    "    DB_URL, DB_USER, DB_PASS = stg_engine()\n",
    "    properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS\n",
    "    }\n",
    "    try:\n",
    "        df.write.jdbc(url = DB_URL,\n",
    "                    table = table_name,\n",
    "                    mode = \"overwrite\",\n",
    "                    properties = properties)\n",
    "        \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "    finally:\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ea69420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to staging\n",
    "from src.staging.load.load_staging import load_staging\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# add column created_at \n",
    "df_user_type = df_user_type.withColumn(\"created_at\", current_timestamp())\n",
    "df_station = df_station.withColumn(\"created_at\", current_timestamp())\n",
    "df_bike = df_bike.withColumn(\"created_at\", current_timestamp())\n",
    "df_trip_2019 = df_trip_2019.withColumn(\"created_at\", current_timestamp())\n",
    "df_trip_2020 = df_trip_2020.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "\n",
    "load_staging(spark, df_user_type, \"user_type\", \"db_bluebikes\")\n",
    "load_staging(spark, df_station, \"station\", \"db_bluebikes\")\n",
    "load_staging(spark, df_bike, \"bike\", \"db_bluebikes\")\n",
    "load_staging(spark, df_trip_2019, \"trip_data_2019\", \"csv\")\n",
    "load_staging(spark, df_trip_2020, \"trip_data_2020\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d26862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Profiling Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e72866",
   "metadata": {},
   "source": [
    "### Building Data Pipeline EL Staging to Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dd322",
   "metadata": {},
   "source": [
    "Solution: \n",
    "1. Pattern: ETL\n",
    "    - `Data Extraction` involves retrieving data from staging area\n",
    "    - `Data Transformation` involve transformed data to fit the desired format or data warehouse structure.\n",
    "    - `Data Validation`  the transformed data is checked for accuracy, completeness, and consistency.\n",
    "    - `Data Loading` involves transferring this staging data into data warehouse\n",
    "\n",
    "2. Data Extraction:\n",
    "    - Sources: Extract data from staging area.\n",
    "    - Techniques: Full Ingestion \n",
    "3. Data Load:\n",
    "    - Data Warehouse: Load clean, transformed and valid data to the final destination.\n",
    "    - Techniques: truncate table and isi ulang\n",
    "\n",
    "4. Data Transformation:\n",
    "    - Transformation: fit the desired format or data warehouse structure.\n",
    "    - Techniques: Joining, Filtering, Aggregation, Deduplication, Conversion, Structuring, etc\n",
    "\n",
    "5. Data Validation\n",
    "    - Data validation is the process of ensuring that data is accurate, complete, and consistent.\n",
    "    - Techniques: \n",
    "        - check missing values, \n",
    "        - verifying data types, \n",
    "        - performing range checks, \n",
    "        - or applying any other rules or constraints to ensure the quality and integrity of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b2dc7",
   "metadata": {},
   "source": [
    "##### Target Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a704d",
   "metadata": {},
   "source": [
    "\n",
    "In the data warehouse, the target schema will be designed using a `dimensional model`\n",
    "Dimension Table:\n",
    "- dim_date\n",
    "- dim_time\n",
    "- dim_bike\n",
    "- dim_station\n",
    "- dim_user_type\n",
    "\n",
    "Fact Table\n",
    "- fact_trip_data\n",
    "- fact_bike_usage\n",
    "\n",
    "\n",
    "<img src= 'https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/w7_warehouse_-_public.png' width=\"800\"> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d939a76",
   "metadata": {},
   "source": [
    "##### Source to Target Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50917840",
   "metadata": {},
   "source": [
    "\n",
    "Source to Target Mapping Documentation: [Link](https://github.com/Kurikulum-Sekolah-Pacmann/pipeline-clinic/blob/main/target_mapping_warehouse.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01bca7",
   "metadata": {},
   "source": [
    "##### Validation Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd9436",
   "metadata": {},
   "source": [
    "\n",
    "Validation Rule:\n",
    "- Data Trip yang akan masuk ke warehouse adalah data dengan user \"Subscriber\" dan \"tripduration\" yang lebih dari 600 seconds\n",
    "\n",
    "Table bike:\n",
    "- all column is not null\n",
    "- jumlah data bike yang berusia > 8 tahun\n",
    "\n",
    "Table station:\n",
    "- all column is not null\n",
    "\n",
    "Table trip\n",
    "- jumlah data selain user \"Subscriber\" dan \"tripduration\" yang lebih dari 600 seconds\n",
    "\n",
    "    \n",
    "All Data will Load to Warehouse, but every validation report will save as JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e528c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline Warehouse\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fcf26",
   "metadata": {},
   "source": [
    "##### Extract Data From Staging Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7730e9d",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Buat modul extarct_staging pada folder src/warehouse/extarct\n",
    "2. Get Database connection URL from database staging\n",
    "3. Extarct data from specific table \n",
    "4. Save Log Info to database log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af514ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extarct with log\n",
    "from src.utils.helper import load_log, stg_engine\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_staging(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = stg_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "    \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"extraction\", \"success\", \"staging\", table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"extraction\", \"failed\", \"staging\", table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b21332bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from staging\n",
    "from src.warehouse.extract.extract_staging import extract_staging\n",
    "\n",
    "# Extarc Data fro Dmension Table\n",
    "df_user_type = extract_staging(spark, 'user_type')\n",
    "df_station = extract_staging(spark, 'station')\n",
    "df_bike = extract_staging(spark, 'bike')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f4887",
   "metadata": {},
   "source": [
    "##### Load Data to Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033c475",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Buat modul load_warehouse pada folder src/warehouse/load\n",
    "2. Get Database connection URL from database warehouse\n",
    "3. Truncate Target Table\n",
    "4. Load Data to Data Warehouse\n",
    "4. Save Log Info to database log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee4c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_log, wh_engine, wh_engine_sqlalchemy\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "\n",
    "\n",
    "# before load to warehouse, truncate the table with sqlalchemy\n",
    "def load_warehouse(spark: SparkSession, df, table_name, source_name):\n",
    "    current_timestamp = datetime.now()\n",
    "    DB_URL, DB_USER, DB_PASS = wh_engine()\n",
    "    properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS\n",
    "    }\n",
    "    try:\n",
    "        # truncate table with sqlalchemy\n",
    "        conn = wh_engine_sqlalchemy()\n",
    "\n",
    "        with conn.connect() as connection:\n",
    "            # Execute the TRUNCATE TABLE command\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} CASCADE\"))\n",
    "            connection.commit()\n",
    "            connection.close()\n",
    "        conn.dispose()\n",
    "    except Exception as e:\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        load_log(spark, log_msg)\n",
    "    \n",
    "    try:\n",
    "        # load data\n",
    "        df.write.jdbc(url = DB_URL,\n",
    "                    table = table_name,\n",
    "                    mode = \"append\",\n",
    "                    properties = properties)\n",
    "        \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abcf601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.load.load_warehouse import load_warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c61c3",
   "metadata": {},
   "source": [
    "##### Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18484e4",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Buat modul table_name_wh.py pada folder src/warehouse/transform \n",
    "2. Buat modul transformasi untuk tabel yang membutuhkan\n",
    "3. lakukan transformasi data from specific table \n",
    "4. Save Log Info to database log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42184f39",
   "metadata": {},
   "source": [
    "Transform Data user_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d46130",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Table: `user_type`      | Target Table: `dim_user_type`   | Description                    |\n",
    "|--------------------------------|---------------------------------|--------------------------------|\n",
    "| `user_type_id` (int8)          | `user_type_nk` (int4)           | Rename                         |\n",
    "| -                              | `user_type_id` (UUID)           | Default Value                     |\n",
    "| `user_type_name` (text)        | `user_type_name` (varchar)      | Direct Mapping                 |\n",
    "| -                              | `created_at` (timestamp)        | Default Value       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ce2cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_user_type(spark, df):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    try:\n",
    "        # rename column user_type_id to user_type_nk\n",
    "        df = df.withColumnRenamed(\"user_type_id\", \"user_type_nk\")\n",
    "\n",
    "        # drop column created_at\n",
    "        df = df.drop(\"created_at\")\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"user_type\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"user_type\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "    finally:\n",
    "        # load log\n",
    "        print(log_msg.show())\n",
    "        # load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e24f78-7c3d-470c-9b57-3bda4171c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.user_type import transform_user_type\n",
    "\n",
    "user_type_transformed = transform_user_type(spark, df_user_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83ed784e-b040-43ea-8a9b-844ace0f6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|user_type_nk|user_type_name|\n",
      "+------------+--------------+\n",
      "|           0|    Subscriber|\n",
      "|           1|      Customer|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_type_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e32c1bf4-7053-4aaf-b346-6f1c8087486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, user_type_transformed, \"dim_user_type\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0c2fe",
   "metadata": {},
   "source": [
    "Transform Data Bike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb4013",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Table: `bike`          | Target Table: `dim_bike`       | Description                    |\n",
    "|-------------------------------|--------------------------------|--------------------------------|\n",
    "| `bike_id` (int8)              | `bike_nk` (int4)               | Rename                         |\n",
    "|   -                            | `bike_id` (UUID)               | default value                   |\n",
    "| `type` (text)                 | `type` (varchar)               | Direct Mapping                 |\n",
    "| `model` (text)                | `model` (varchar)              | Direct Mapping                 |\n",
    "| `purchase_date` (date)        | `purchase_date` (date)         | Direct Mapping                 |\n",
    "| -    | `created_at` (timestamp)       |  default value  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b04a9cb5-b5a2-432f-a5cc-adf398e68ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_bike(spark, df):\n",
    "    current_timestamp = datetime.now()\n",
    "    try:\n",
    "        # rename column bike_id to bike_nk\n",
    "        df = df.withColumnRenamed(\"bike_id\", \"bike_nk\")\n",
    "\n",
    "        # drop column created_at\n",
    "        df = df.drop(\"created_at\")\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"bike\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"bike\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "819c4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data bike\n",
    "from src.warehouse.transformation.bike import transform_bike\n",
    "\n",
    "bike_transformed = transform_bike(spark, df_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24434bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|bike_nk|    type| model|purchase_date|\n",
      "+-------+--------+------+-------------+\n",
      "|      1|    road|R88W4N|   2020-12-02|\n",
      "|      4|    road|G79YM9|   2022-10-08|\n",
      "|      7|foldable|IOEZRL|   2024-07-07|\n",
      "|      8|foldable|ADQ2DA|   2021-03-26|\n",
      "|      9|foldable|8EEQ8Z|   2023-03-04|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d67a049-dfa0-4dc4-818f-3ac095df51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, bike_transformed, \"dim_bike\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c79a84",
   "metadata": {},
   "source": [
    "Transform Data Station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52466d1",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Table: `station`   | Target Table: `dim_station`   |Description                 |\n",
    "|---------------------------|-------------------------------|-------------------------------|\n",
    "| `station_id` (int8)       | `station_nk` (int4)           | Rename from `station_id` to `station_nk` |\n",
    "|                           | `station_id` (UUID)           |Default Value |\n",
    "| `station_name` (text)     | `station_name` (varchar)      | Direct Mapping                   |\n",
    "| `latitude` (float8)       | `latitude` (numeric)          | Direct Mapping |\n",
    "| `longitude` (float8)      | `longitude` (numeric)         | Direct Mapping |\n",
    "| `created_at` (timestamp)  | `created_at` (timestamp)      | Default Value |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57a811b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_station(spark, df):\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # rename column station_id to station_nk\n",
    "        df = df.withColumnRenamed(\"station_id\", \"station_nk\")\n",
    "\n",
    "        # drop column created_at\n",
    "        df = df.drop(\"created_at\")\n",
    "      \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"station\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"station\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7943bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.station import transform_station\n",
    "\n",
    "station_transformed = transform_station(spark, df_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b891553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+\n",
      "|station_nk|        station_name|          latitude|         longitude|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "|         1|18 Dorrance Wareh...|         42.387151|        -71.075978|\n",
      "|         3|Colleges of the F...| 42.34011512249237|-71.10061883926393|\n",
      "|         4|Tremont St at E B...|         42.345392|        -71.069616|\n",
      "|         5|Northeastern Univ...|         42.341814|        -71.090179|\n",
      "|         6|Cambridge St at J...|42.361211653079856|-71.06530619789737|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d94ac2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, station_transformed, \"dim_station\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620e963",
   "metadata": {},
   "source": [
    "Data for Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4243cab",
   "metadata": {},
   "source": [
    "You can extract semua data trip_2019 and trip_2020, tapi karena kita hanya memerlukan data trip dengan user type \"Subscriber\", maka kita bisa melkukan extrack dan filter data\n",
    "\n",
    "Buat fungsi extarct_staging_filter() pada modul src/warehouse/extract_staging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c2b724fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_staging_filter(spark: SparkSession, table_name, filter_column, filter_value):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = stg_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\\\n",
    "                .filter(f\"{filter_column} = '{filter_value}'\")\n",
    "    \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"extraction\", \"success\", \"staging\", table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"extraction\", \"failed\", \"staging\", table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c9fdfd-0077-4562-9349-6d98cbfdb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extarct data for Fact Table\n",
    "from src.warehouse.extract.extract_staging import extract_staging, extract_staging_filter\n",
    "\n",
    "df_trip_2019 = extract_staging_filter(spark, 'trip_data_2019',\"usertype\", \"Subscriber\")\n",
    "df_trip_2020 = extract_staging_filter(spark, 'trip_data_2020',\"usertype\", \"Subscriber\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e25eb",
   "metadata": {},
   "source": [
    "Transfrom data fact_trip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd97f6",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Tables: `trip_data_2019`, `trip_data_2020` | Target Table: `fact_trip_data` | **Description**                              |\n",
    "|--------------------------------------------------|-------------------------------|-------------------------------                 |\n",
    "| `tripduration` (text)                             | `trip_duration` (int4)        | Data Type Conversion                          |\n",
    "| `starttime` (text)                                | `start_date` (int4)           | Convert Date Part Lookup to `dim_date`        |\n",
    "| `starttime` (text)                                | `start_time` (int4)           | Convert Time Part Lookup to `dim_time`        |\n",
    "| `stoptime` (text)                                 | `stop_date` (int4)            | Convert Date Part Lookup to `dim_date`        |\n",
    "| `stoptime` (text)                                 | `stop_time` (int4)            | Convert Time Part Lookup to `dim_time`        |\n",
    "| `\"start station id\"` (text)                       | `start_station_id` (uuid)     | Lookup to `dim_station`, Data Type Conversion |\n",
    "| `\"end station id\"` (text)                         | `end_station_id` (uuid)       | Lookup to `dim_station`, Data Type Conversion |\n",
    "| `\"start station name\"` (text)                     | -                             | Not Mapped |\n",
    "| `\"start station latitude\"` (text)                 | -                             | Not Mapped |\n",
    "| `\"start station longitude\"` (text)                | -                             | Not Mapped |\n",
    "| `\"end station name\"` (text)                       | -                             | Not Mapped |\n",
    "| `\"end station latitude\"` (text)                   | -                             | Not Mapped |\n",
    "| `\"end station longitude\"` (text)                  | -                             | Not Mapped |\n",
    "| `bikeid` (text)                                   | `bike_id` (uuid)              | Lookup to `dim_bike`, Data Type Conversion    |\n",
    "| `usertype` (text)                                 | `user_type_id` (uuid)         | Lookup to `dim_user_type`, Data Type Conversion |\n",
    "| `year` (text)                                     | `year` (varchar)              | Direct Mapping                                |\n",
    "| `month` (text)                                    | `month` (varchar)             | Direct Mapping                                |\n",
    "| `postal code` (text)                              | -                             | Not Mapped                                    |\n",
    "| `birth year` (text)                               | -                             | Not Mapped                                    |\n",
    "| `gender` (text)                                   | -                             | Not Mapped                                    |\n",
    "| `created_at` (timestamp)                          | -                             | Default Value                                 |\n",
    "| -                                                 | `trip_id` (uuid)              | Generated UUID                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25fb30",
   "metadata": {},
   "source": [
    "Create Modul Extarct data Dimension table in data warehouse, karena kita memerlukan foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "96cd0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extarct with log\n",
    "from src.utils.helper import wh_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_warehouse(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = wh_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4db88ee7-8e24-4eef-ba1e-ee75ca3e4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import date_format, col\n",
    "from src.warehouse.extract.extract_warehouse import extract_warehouse\n",
    "\n",
    "def transform_fact_trip(spark: SparkSession, df):\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extarct data dimension table\n",
    "        df_user_type = extract_warehouse(spark, \"dim_user_type\", [\"user_type_id\", \"user_type_name\"])\n",
    "        df_station = extract_warehouse(spark, \"dim_station\", [\"station_id\", \"station_nk\"])\n",
    "        df_bike = extract_warehouse(spark, \"dim_bike\", [\"bike_id\", \"bike_nk\"])\n",
    "        df_date = extract_warehouse(spark, \"dim_date\", [\"date_id\", \"date_actual\"])\n",
    "        df_time = extract_warehouse(spark, \"dim_time\", [\"time_id\", \"time_actual\"])\n",
    "\n",
    "        # convert df_time column time_actual to HH:mm:ss\n",
    "        df_time = df_time.withColumn(\"time_actual\", date_format(col(\"time_actual\"),  \"HH:mm:ss\"))\n",
    "\n",
    "        #conver tripduration to int and filter tripduration > 600\n",
    "        df = df.withColumn(\"tripduration\", df[\"tripduration\"].cast(\"int\")) \\\n",
    "                .filter(df[\"tripduration\"] > 600)\n",
    "\n",
    "        # rename column\n",
    "        df = df.withColumnRenamed(\"tripduration\", \"trip_duration\") \\\n",
    "                .withColumnRenamed(\"start station id\", \"start_station_nk\") \\\n",
    "                .withColumnRenamed(\"end station id\", \"end_station_nk\") \\\n",
    "                .withColumnRenamed(\"bikeid\", \"bike_nk\") \\\n",
    "                .withColumnRenamed(\"usertype\", \"user_type_name\")\n",
    "    \n",
    "        \n",
    "        # extract date and time from starttime and stoptime\n",
    "        df = df.withColumn(\"start_date_temp\", date_format(col(\"starttime\"),  \"yyyy-MM-dd\")) \\\n",
    "                .withColumn(\"start_time_temp\", date_format(col(\"starttime\"),  \"HH:mm:00\")) \\\n",
    "                .withColumn(\"stop_date_temp\", date_format(col(\"stoptime\"),  \"yyyy-MM-dd\")) \\\n",
    "                .withColumn(\"stop_time_temp\", date_format(col(\"stoptime\"),  \"HH:mm:00\"))\n",
    "        \n",
    "        # get bike_id from dim_bike\n",
    "        df = df.join(df_bike, df.bike_nk == df_bike.bike_nk, \"left\") \\\n",
    "                .drop(df_bike.bike_nk)\n",
    "        \n",
    "        # get user_type_id from dim_user_type\n",
    "        df = df.join(df_user_type, df.user_type_name == df_user_type.user_type_name, \"left\") \\\n",
    "                .drop(df_user_type.user_type_name)\n",
    "        \n",
    "        # get start_station_id from dim_station\n",
    "        df = df.join(df_station, df.start_station_nk == df_station.station_nk, \"left\") \\\n",
    "                .drop(df_station.station_nk)\n",
    "        # rename column station_id to start_station_id\n",
    "        df = df.withColumnRenamed(\"station_id\", \"start_station_id\")\n",
    "\n",
    "        # get end_station_id from dim_station\n",
    "        df = df.join(df_station, df.end_station_nk == df_station.station_nk, \"left\") \\\n",
    "                .drop(df_station.station_nk)\n",
    "        # rename column station_id to end_station_id\n",
    "        df = df.withColumnRenamed(\"station_id\", \"end_station_id\")\n",
    "\n",
    "        # get date_id from dim_date\n",
    "        df = df.join(df_date, df.start_date_temp == df_date.date_actual, \"left\") \\\n",
    "                .drop(df_date.date_actual)\n",
    "        # rename column date_id to start_date\n",
    "        df = df.withColumnRenamed(\"date_id\", \"start_date\")\n",
    "\n",
    "        # get date_id from dim_date\n",
    "        df = df.join(df_date, df.stop_date_temp == df_date.date_actual, \"left\") \\\n",
    "                .drop(df_date.date_actual)\n",
    "        # rename column date_id to stop_date\n",
    "        df = df.withColumnRenamed(\"date_id\", \"stop_date\")\n",
    "\n",
    "        # get time_id from dim_time\n",
    "        df = df.join(df_time, df.start_time_temp == df_time.time_actual, \"left\") \\\n",
    "                .drop(df_time.time_actual)\n",
    "        # rename column time_id to start_time\n",
    "        df = df.withColumnRenamed(\"time_id\", \"start_time\")\n",
    "\n",
    "        # get time_id from dim_time\n",
    "        df = df.join(df_time, df.stop_time_temp == df_time.time_actual, \"left\") \\\n",
    "                .drop(df_time.time_actual)\n",
    "        # rename column time_id to stop_time\n",
    "        df = df.withColumnRenamed(\"time_id\", \"stop_time\")\n",
    "\n",
    "        \n",
    "        # drop unnecessary column created_at, postal code, birth year, start station name, end station name, \n",
    "        # start station latitude, start station longitude, end station latitude, end station longitude,\n",
    "        # bike_nk, user_type_name, start_station_nk, end_station_nk\n",
    "        if \"postal code\" in df.columns:\n",
    "            df = df.drop(\"postal code\")\n",
    "\n",
    "        df = df.drop(\"created_at\", \"birth year\", \"start station name\", \"end station name\", \"gender\",\n",
    "                    \"start station latitude\", \"start station longitude\", \"end station latitude\", \"end station longitude\",\n",
    "                    \"bike_nk\", \"user_type_name\", \"start_station_nk\", \"end_station_nk\", \"starttime\",\"stoptime\", \"station_nk\",\n",
    "                    \"start_date_temp\",\"stop_date_temp\", \"start_time_temp\",\"stop_time_temp\", \"date_actual\", \"time_actual\")\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"trip_data\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"trip_data\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fee966f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.fact_trip import transform_fact_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea1c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_2019_transformed = transform_fact_trip(spark, df_trip_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1a5830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trip_duration', 'int'),\n",
       " ('year', 'string'),\n",
       " ('month', 'string'),\n",
       " ('bike_id', 'bigint'),\n",
       " ('user_type_id', 'bigint'),\n",
       " ('start_station_id', 'bigint'),\n",
       " ('end_station_id', 'bigint'),\n",
       " ('start_date', 'int'),\n",
       " ('stop_date', 'int'),\n",
       " ('start_time', 'int'),\n",
       " ('stop_time', 'int')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show column name\n",
    "trip_2019_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c21a74ad-ed3c-456e-84bb-7f75e24ce354",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_2020_transformed = transform_fact_trip(spark, df_trip_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94c4ed9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trip_duration', 'int'),\n",
       " ('year', 'string'),\n",
       " ('month', 'string'),\n",
       " ('bike_id', 'bigint'),\n",
       " ('user_type_id', 'bigint'),\n",
       " ('start_station_id', 'bigint'),\n",
       " ('end_station_id', 'bigint'),\n",
       " ('start_date', 'int'),\n",
       " ('stop_date', 'int'),\n",
       " ('start_time', 'int'),\n",
       " ('stop_time', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_2020_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86958fcc-f110-4480-a3a5-36f6ab7b41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # join data trip 2019 and 2020\n",
    "trip_union = trip_2019_transformed.unionByName(trip_2020_transformed, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef290e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trip_duration', 'int'),\n",
       " ('year', 'string'),\n",
       " ('month', 'string'),\n",
       " ('bike_id', 'bigint'),\n",
       " ('user_type_id', 'bigint'),\n",
       " ('start_station_id', 'bigint'),\n",
       " ('end_station_id', 'bigint'),\n",
       " ('start_date', 'int'),\n",
       " ('stop_date', 'int'),\n",
       " ('start_time', 'int'),\n",
       " ('stop_time', 'int')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_union.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eaf0579-be6e-4c88-9ceb-9f1539e97d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 35046)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'log_msg' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/work/src/warehouse/load/load_warehouse.py:34\u001b[0m, in \u001b[0;36mload_warehouse\u001b[0;34m(spark, df, table_name, source_name)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# load data\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDB_URL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m                \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m#log message\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/readwriter.py:1984\u001b[0m, in \u001b[0;36mDataFrameWriter.jdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1983\u001b[0m     jprop\u001b[38;5;241m.\u001b[39msetProperty(k, properties[k])\n\u001b[0;32m-> 1984\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjdbc\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjprop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o556.jdbc",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/work/src/warehouse/load/load_warehouse.py:49\u001b[0m, in \u001b[0;36mload_warehouse\u001b[0;34m(spark, df, table_name, source_name)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# print(e)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \n\u001b[1;32m     47\u001b[0m     \u001b[38;5;66;03m# log message\u001b[39;00m\n\u001b[1;32m     48\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparkContext\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m---> 49\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallelize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwarehouse\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mload\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfailed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_timestamp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\\\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;241m.\u001b[39mtoDF([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124metl_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_msg\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:783\u001b[0m, in \u001b[0;36mSparkContext.parallelize\u001b[0;34m(self, c, numSlices)\u001b[0m\n\u001b[1;32m    752\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    753\u001b[0m \u001b[38;5;124;03mDistribute a local Python collection to form an RDD. Using range\u001b[39;00m\n\u001b[1;32m    754\u001b[0m \u001b[38;5;124;03mis recommended if the input represents a range for performance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m[['a'], ['b', 'c']]\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 783\u001b[0m numSlices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(numSlices) \u001b[38;5;28;01mif\u001b[39;00m numSlices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefaultParallelism\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mrange\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/context.py:630\u001b[0m, in \u001b[0;36mSparkContext.defaultParallelism\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    621\u001b[0m \u001b[38;5;124;03mDefault level of parallelism to use when not given by user (e.g. for reduce tasks)\u001b[39;00m\n\u001b[1;32m    622\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;124;03mTrue\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdefaultParallelism()\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m   called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m   :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data to warehouse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mload_warehouse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrip_union\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfact_trip_data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstaging\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/src/warehouse/load/load_warehouse.py:53\u001b[0m, in \u001b[0;36mload_warehouse\u001b[0;34m(spark, df, table_name, source_name)\u001b[0m\n\u001b[1;32m     48\u001b[0m     log_msg \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\\\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;241m.\u001b[39mparallelize([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailed\u001b[39m\u001b[38;5;124m\"\u001b[39m, source_name, table_name, current_timestamp, \u001b[38;5;28mstr\u001b[39m(e))])\\\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;241m.\u001b[39mtoDF([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprocess\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtable_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124metl_date\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_msg\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m---> 53\u001b[0m     load_log(spark, \u001b[43mlog_msg\u001b[49m)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'log_msg' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, trip_union, \"fact_trip_data\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041207c6",
   "metadata": {},
   "source": [
    "Transform data fact_bike_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c296b8",
   "metadata": {},
   "source": [
    "data fact_bike_usage didapatkan dari agregasi data fact_trip_data\n",
    "mencai untuk setiap bike, berapa total trip duration dan trip yang pernah dilakukan\n",
    "\n",
    "step:\n",
    "1. Extarct data fact_trip_data\n",
    "2. Group BY bike_id\n",
    "3. Count trip_id\n",
    "4. Sum trip_duration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7fb299ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import date_format, col\n",
    "from src.warehouse.extract.extract_warehouse import extract_warehouse\n",
    "\n",
    "def transform_fact_bike_usage(spark: SparkSession):\n",
    "        current_timestamp = datetime.now()\n",
    "\n",
    "        try:\n",
    "                # Extarct data fact_trip_data\n",
    "                columns = ['trip_duration', 'trip_id', 'bike_id']\n",
    "\n",
    "                df_trip = extract_warehouse(spark, \"fact_trip_data\", columns)\n",
    "\n",
    "                # group by bike_id and count trip_id, sum trip_duration\n",
    "                df_bike_usage = df_trip.groupBy(\"bike_id\") \\\n",
    "                                .agg({\"trip_duration\": \"sum\", \"trip_id\": \"count\"}) \\\n",
    "                                .withColumnRenamed(\"sum(trip_duration)\", \"total_duration\") \\\n",
    "                                .withColumnRenamed(\"count(trip_id)\", \"trip_count\")\n",
    "                \n",
    "                #log message\n",
    "                log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fact_bike_usage\", current_timestamp)])\\\n",
    "                .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "                return df_bike_usage\n",
    "        except Exception as e:\n",
    "                #log message\n",
    "                print(e)\n",
    "                log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fact_bike_usage\", current_timestamp, str(e))])\\\n",
    "                .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf9f1560-023c-4d9c-a57c-609f5a22c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.fact_bike_usage import transform_fact_bike_usage\n",
    "fact_bike_usage = transform_fact_bike_usage(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ede8f90-865f-4d80-be0a-0bb132af3045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bike_id', 'trip_count', 'total_duration']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_bike_usage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ba4a8a7-70a4-4f31-afee-3619fb8dfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, fact_bike_usage, \"fact_bike_usage\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c69b658",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed76bfb-b678-4ebd-9213-546513912aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
