{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00e77c36",
   "metadata": {},
   "source": [
    "# Week #7 - Live Class\n",
    "Data Pipeline Course - Sekolah Engineer - Pacmann Academy \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44aa83",
   "metadata": {},
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cfda11",
   "metadata": {},
   "source": [
    "Objective:\n",
    "1. Create Data Pipeline for integrating bluebikes data with pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7be6c5",
   "metadata": {},
   "source": [
    "## Case Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa0a4e9",
   "metadata": {},
   "source": [
    "1. `Problem`\n",
    "\n",
    "A company named Bluebike wants to analyze subscriber data who have used their services. Transaction data for trips is still stored in CSV files for each year, while object data such as stations and bikes are in a database.\n",
    "\n",
    "- **CSV Files**: Trip data conducted for each year (2019 and 2020).\n",
    "- **BlueBikes**: Object data for stations and bikes from Bluebikes.\n",
    "\n",
    "2. `Solution`\n",
    "\n",
    "To address these issues, an ETL (Extract, Transform, Load) pipeline will be developed. This pipeline will extract data from different sources, apply necessary transformations to clean and standardize the data, and then load it into a unified data warehouse. The pipeline will consist of two layers: Staging and Warehouse, and will include a logging system.\n",
    "\n",
    "![Pipeline Diagram](https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/bluebikes_pipeline.drawio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef97357",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c234245e",
   "metadata": {},
   "source": [
    "`Docker Compose` and `repository`:\n",
    "<br> You can find the Docker Compose configuration and the repository at the following link:\n",
    "- [Pipeline Bluebikes Repository](https://github.com/Kurikulum-Sekolah-Pacmann/pipeline-bluebikes.git)\n",
    "\n",
    "`Source Dataset` csv: \n",
    "The source dataset for trip data is available as a CSV file:\n",
    "- **Bluebikes Trip Data CSV**: [Link to Dataset](https://www.kaggle.com/datasets/jackdaoud/bluebikes-in-boston)\n",
    "<br>copy in directory: ./script/data\n",
    "\n",
    "`Tools and Technologies`:\n",
    "- Python and Pyspark: For build Data Pipeline\n",
    "- PostgreSQL: For log, staging and final data storage\n",
    "- Docker: For running MinIO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7055660",
   "metadata": {},
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c92358",
   "metadata": {},
   "source": [
    "1. `Profiling` <br>\n",
    "Profiling involves analyzing and understanding the structure, content, and quality of the data from multiple sources within the clinic\n",
    "\n",
    "2. `Building Data Pipeline EL Source to Staging` <br>\n",
    "This step focuses on extracting data from the blubikes various source systems. The extracted data is then loaded into a staging area\n",
    "\n",
    "3. `Building Data Pipeline ETL Staging to Warehouse` <br>\n",
    "In this phase, data from the staging area is transformed and loaded into the data warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0ff1bb",
   "metadata": {},
   "source": [
    "### Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92e369",
   "metadata": {},
   "source": [
    "In this task, you will conduct profiling of each table or file provided in the dataset. The profiling process involves the following steps:\n",
    "\n",
    "1. **Check Number of Columns and Column Names**  \n",
    "   Verify the number of columns and their names in each table or file to ensure they match the expected schema.\n",
    "\n",
    "2. **Check Number of Rows**  \n",
    "   Count the total number of rows to understand the size of the dataset and identify any discrepancies.\n",
    "\n",
    "3. **Check Data Types**  \n",
    "   Examine the data types of each column to confirm they align with the expected types and identify any inconsistencies.\n",
    "\n",
    "4. **Check Percentage of Missing Values**  \n",
    "   Calculate the percentage of missing values in each column to assess data completeness and quality.\n",
    "\n",
    "5. **Check Percentage of Valid Date Formats**  \n",
    "   Determine the percentage of valid date formats in date columns to ensure data consistency and validity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26c18f",
   "metadata": {},
   "source": [
    "The first step in the profiling process is to `extract the data` from the source systems. This involves gathering data from various tables or csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5456473",
   "metadata": {},
   "source": [
    "`Implementing Modular Code in ETL PySpark Process`\n",
    "\n",
    "When implementing modular code in your ETL PySpark process, it is essential to pass the Spark session as a parameter to your functions. This practice ensures that you remain within the same Spark session while executing related functions, maintaining consistency across your ETL operations.\n",
    "\n",
    "For example, if your main file is `pipeline_staging.py`, and you import the `extract_database()` function from `src.staging.extract`, you should pass the Spark session to `extract_database(spark_seesion)` when calling it, especially if the function is use `spark operation`. This approach keeps the function **within the Spark session** created in your main file, ensuring that all operations are performed within the same session context.\n",
    "![Spark Session Diagram](https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/sparksession.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8dbf41",
   "metadata": {},
   "source": [
    "#### Load Your .env file\n",
    "\n",
    "create `helper.py` to create your database connection (src/utils)\n",
    "\n",
    "example of .env file:\n",
    "\n",
    "```\n",
    "DB_HOST_SOURCE=\"localhost\"\n",
    "DB_USER_SOURCE=\"postgres\"\n",
    "DB_PASS_SOURCE=\"aku\"\n",
    "DB_PORT_SOURCE=\"5432\"\n",
    "\n",
    "DB_HOST_TARGET=\"localhost\"\n",
    "DB_USER_TARGET=\"postgres\"\n",
    "DB_PASS_TARGET=\"aku\"\n",
    "DB_PORT_TARGET=\"5432\"\n",
    "\n",
    "\n",
    "DB_NAME_BLUEBIKES=\"bluebikes\"\n",
    "DB_NAME_STG=\"staging\"\n",
    "DB_NAME_LOG=\"etl_log\"\n",
    "DB_NAME_WH=\"warehouse\"\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff3f1a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "DB_HOST_SOURCE = os.getenv(\"DB_HOST_SOURCE\")\n",
    "DB_USER_SOURCE = os.getenv(\"DB_USER_SOURCE\")\n",
    "DB_PASS_SOURCE = os.getenv(\"DB_PASS_SOURCE\")\n",
    "DB_PORT_SOURCE = os.getenv(\"DB_PORT_SOURCE\")\n",
    "\n",
    "DB_HOST_TARGET = os.getenv(\"DB_HOST_TARGET\")\n",
    "DB_USER_TARGET = os.getenv(\"DB_USER_TARGET\")\n",
    "DB_PASS_TARGET = os.getenv(\"DB_PASS_TARGET\")\n",
    "DB_PORT_TARGET = os.getenv(\"DB_PORT_TARGET\")\n",
    "\n",
    "DB_NAME_BLUEBIKES = os.getenv(\"DB_NAME_BLUEBIKES\")\n",
    "DB_NAME_STG = os.getenv(\"DB_NAME_STG\")\n",
    "DB_NAME_LOG = os.getenv(\"DB_NAME_LOG\")\n",
    "DB_NAME_WH = os.getenv(\"DB_NAME_WH\")\n",
    "\n",
    "# Create URL link for each database connection\n",
    "\n",
    "def bluebikes_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_SOURCE}:{DB_PORT_SOURCE}/{DB_NAME_BLUEBIKES}\"\n",
    "    return DB_URL, DB_USER_SOURCE, DB_PASS_SOURCE\n",
    "\n",
    "def stg_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_STG}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET\n",
    "\n",
    "def log_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_LOG}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET\n",
    "\n",
    "def wh_engine():\n",
    "    DB_URL = f\"jdbc:postgresql://{DB_HOST_TARGET}:{DB_PORT_TARGET}/{DB_NAME_WH}\"\n",
    "    return DB_URL, DB_USER_TARGET, DB_PASS_TARGET\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e7d4d5",
   "metadata": {},
   "source": [
    "#### Extarct Data From Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f041df67",
   "metadata": {},
   "source": [
    "Create a file named `extract.py` in the `src/profiling/` directory to store the functions `extract_database` and `extract_csv`. \n",
    "\n",
    "Hereâ€™s how you can define these functions:\n",
    "\n",
    "1. **`extract_database`**: This function will extract data from a database using a Spark session.\n",
    "2. **`extract_csv`**: This function will extract data from a CSV file using a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c33ebbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import bluebikes_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_database(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = bluebikes_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    # read data\n",
    "    df = spark \\\n",
    "              .read \\\n",
    "              .jdbc(url = DB_URL,\n",
    "                    table = table_name,\n",
    "                    properties = connection_properties)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e792adf",
   "metadata": {},
   "source": [
    "#### Extarct Data From CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eeae7df-dc3c-43a9-8ad7-1a0ea61d0637",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"data/\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_csv(spark: SparkSession, file_name):\n",
    "\n",
    "    # read data\n",
    "    df = spark.read.csv(PATH + file_name, header=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15213c61-3cfd-458e-9b36-8c193cc8a6d2",
   "metadata": {},
   "source": [
    "#### Profiling Data From Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71a4b86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b62de9b-d293-4d7b-8a43-19db64af208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Profiling Data\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95a068",
   "metadata": {},
   "source": [
    "Extarct Data Table bike, station and user_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0704692-7fdf-4bd8-862c-b354952a11cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.profiling.extract import extract_database, extract_csv\n",
    "\n",
    "df_user_type = extract_database(spark, 'user_type')\n",
    "df_station = extract_database(spark, 'station')\n",
    "df_bike = extract_database(spark, 'bike')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a113819-0655-40dd-a1db-1cfe3ce12ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|user_type_id|user_type_name|\n",
      "+------------+--------------+\n",
      "|           0|    Subscriber|\n",
      "|           1|      Customer|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5ab31f-368b-4da1-b856-bcb72afbbae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+\n",
      "|station_id|        station_name|          latitude|         longitude|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "|         1|18 Dorrance Wareh...|         42.387151|        -71.075978|\n",
      "|         3|Colleges of the F...| 42.34011512249237|-71.10061883926393|\n",
      "|         4|Tremont St at E B...|         42.345392|        -71.069616|\n",
      "|         5|Northeastern Univ...|         42.341814|        -71.090179|\n",
      "|         6|Cambridge St at J...|42.361211653079856|-71.06530619789737|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_station.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3eea7e9-ae22-4a79-8cac-a5b31e3ab2c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|bike_id|    type| model|purchase_date|\n",
      "+-------+--------+------+-------------+\n",
      "|      1|    road|R88W4N|   2020-12-02|\n",
      "|      4|    road|G79YM9|   2022-10-08|\n",
      "|      7|foldable|IOEZRL|   2024-07-07|\n",
      "|      8|foldable|ADQ2DA|   2021-03-26|\n",
      "|      9|foldable|8EEQ8Z|   2023-03-04|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bike.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80eb0820",
   "metadata": {},
   "source": [
    "Extract Data CSV bluebikes_tripdata_2019.csv and bluebikes_tripdata_2020.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23097ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_2020 = extract_csv(spark, 'bluebikes_tripdata_2020.csv')\n",
    "df_trip_2019 = extract_csv(spark, 'bluebikes_tripdata_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff68e592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|postal code|year|month|birth year|gender|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|        1793|2020-11-01 00:00:...|2020-11-01 00:30:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  4896|  Customer|      11214|2020|   11|      NULL|  NULL|\n",
      "|        1832|2020-11-01 00:00:...|2020-11-01 00:31:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  5630|  Customer|      11220|2020|   11|      NULL|  NULL|\n",
      "|         262|2020-11-01 00:01:...|2020-11-01 00:06:...|             186|Congress St at No...|               42.3481|              -71.03764|             7|            Fan Pier|  42.353390507052296|   -71.04457139968872|  5634|Subscriber|      02128|2020|   11|      NULL|  NULL|\n",
      "|         419|2020-11-01 00:04:...|2020-11-01 00:10:...|              74|Harvard Square at...|    42.373267999999996|             -71.118579|            76|Central Sq Post O...|  42.366426000000004|   -71.10549499999999|  6071|  Customer|      02139|2020|   11|      NULL|  NULL|\n",
      "|         275|2020-11-01 00:04:...|2020-11-01 00:08:...|              73|Harvard Square at...|             42.373231|             -71.120886|           104|Harvard Universit...|           42.380287|           -71.125107|  2712|Subscriber|      21015|2020|   11|      NULL|  NULL|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|year|month|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|         790|2019-12-01 00:01:...|2019-12-01 00:14:...|             370|Dartmouth St at N...|     42.35096144421219|     -71.07782810926437|            33|      Kenmore Square|           42.348706|           -71.097009|  5133|Subscriber|      1950|     1|2019|   12|\n",
      "|         166|2019-12-01 00:05:...|2019-12-01 00:08:...|              80|MIT Stata Center ...|      42.3621312344991|     -71.09115600585936|            67|MIT at Mass Ave /...|             42.3581|           -71.093198|  2653|Subscriber|      1993|     1|2019|   12|\n",
      "|         323|2019-12-01 00:08:...|2019-12-01 00:13:...|             381|Inman Square at S...|     42.37438408515815|      -71.1001574621514|           221|Verizon Innovatio...|   42.37250864997261|   -71.11305356025694|  4875|Subscriber|      1992|     1|2019|   12|\n",
      "|         709|2019-12-01 00:08:...|2019-12-01 00:20:...|             185|     Third at Binney|    42.365444861373994|      -71.0827714204788|           184|Sidney Research C...|   42.35775309465199|   -71.10393404960631|  2116|Subscriber|      1997|     1|2019|   12|\n",
      "|         332|2019-12-01 00:10:...|2019-12-01 00:15:...|             221|Verizon Innovatio...|     42.37250864997261|     -71.11305356025694|            89|Harvard Law Schoo...|           42.379011|           -71.119945|  6156|Subscriber|      1985|     1|2019|   12|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_2020.show(5)\n",
    "df_trip_2019.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2812930",
   "metadata": {},
   "source": [
    "##### 1. Check the column size and name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4300bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Column Count and Column Name ---\n",
      "Column in data user_type: ['user_type_id', 'user_type_name'], Count: 2\n",
      "Column in data station: ['station_id', 'station_name', 'latitude', 'longitude'], Count: 4\n",
      "Column in data bike: ['bike_id', 'type', 'model', 'purchase_date'], Count: 4\n",
      "Column in data trip_2020: ['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', 'start station latitude', 'start station longitude', 'end station id', 'end station name', 'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'postal code', 'year', 'month', 'birth year', 'gender'],  Count: 18\n",
      "Column in data trip_2019: ['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', 'start station latitude', 'start station longitude', 'end station id', 'end station name', 'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender', 'year', 'month'], Count: 17\n",
      "--- Data Size ---\n",
      "Data size user_type: 2\n",
      "Data size station: 387\n",
      "Data size bike: 4989\n",
      "Data size trip_2020: 1999446\n",
      "Data size trip_2019: 2522771\n"
     ]
    }
   ],
   "source": [
    "# show column count and column name\n",
    "print(\"--- Column Count and Column Name ---\")\n",
    "print(f\"Column in data user_type: {df_user_type.columns}, Count: {len(df_user_type.columns)}\")\n",
    "print(f\"Column in data station: {df_station.columns}, Count: {len(df_station.columns)}\")\n",
    "print(f\"Column in data bike: {df_bike.columns}, Count: {len(df_bike.columns)}\")\n",
    "print(f\"Column in data trip_2020: {df_trip_2020.columns},  Count: {len(df_trip_2020.columns)}\")\n",
    "print(f\"Column in data trip_2019: {df_trip_2019.columns}, Count: {len(df_trip_2019.columns)}\")\n",
    "\n",
    "# show data size\n",
    "print(\"--- Data Size ---\")\n",
    "print(f\"Data size user_type: {df_user_type.count()}\")\n",
    "print(f\"Data size station: {df_station.count()}\")\n",
    "print(f\"Data size bike: {df_bike.count()}\")\n",
    "print(f\"Data size trip_2020: {df_trip_2020.count()}\")\n",
    "print(f\"Data size trip_2019: {df_trip_2019.count()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0552e825",
   "metadata": {},
   "source": [
    "- We observed that the data for trips in 2020 and 2019 have different column counts. The 2020 data includes a `postal code` column, which is absent in the 2019 data.\n",
    "- The user data available for trips is limited to 'usertype', 'birth year', and 'gender'. The database does not contain additional user information.\n",
    "\n",
    "Next Steps: **Determine Data Types for Each Column**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b16051",
   "metadata": {},
   "source": [
    "##### 2. Check the data size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bae86c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Type For Each Column---\n",
      "Data type for each column in user_type: [('user_type_id', 'bigint'), ('user_type_name', 'string')]\n",
      "Data type for each column in station: [('station_id', 'bigint'), ('station_name', 'string'), ('latitude', 'double'), ('longitude', 'double')]\n",
      "Data type for each column in bike: [('bike_id', 'bigint'), ('type', 'string'), ('model', 'string'), ('purchase_date', 'date')]\n",
      "Data type for each column in trip_2020: [('tripduration', 'string'), ('starttime', 'string'), ('stoptime', 'string'), ('start station id', 'string'), ('start station name', 'string'), ('start station latitude', 'string'), ('start station longitude', 'string'), ('end station id', 'string'), ('end station name', 'string'), ('end station latitude', 'string'), ('end station longitude', 'string'), ('bikeid', 'string'), ('usertype', 'string'), ('postal code', 'string'), ('year', 'string'), ('month', 'string'), ('birth year', 'string'), ('gender', 'string')]\n",
      "Data type for each column in trip_2019: [('tripduration', 'string'), ('starttime', 'string'), ('stoptime', 'string'), ('start station id', 'string'), ('start station name', 'string'), ('start station latitude', 'string'), ('start station longitude', 'string'), ('end station id', 'string'), ('end station name', 'string'), ('end station latitude', 'string'), ('end station longitude', 'string'), ('bikeid', 'string'), ('usertype', 'string'), ('birth year', 'string'), ('gender', 'string'), ('year', 'string'), ('month', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# get data type of each column\n",
    "print(\"--- Data Type For Each Column---\")\n",
    "print(f\"Data type for each column in user_type: {df_user_type.dtypes}\")\n",
    "print(f\"Data type for each column in station: {df_station.dtypes}\")\n",
    "print(f\"Data type for each column in bike: {df_bike.dtypes}\")\n",
    "print(f\"Data type for each column in trip_2020: {df_trip_2020.dtypes}\")\n",
    "print(f\"Data type for each column in trip_2019: {df_trip_2019.dtypes}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503633d",
   "metadata": {},
   "source": [
    "- We observed that the data extracted from the CSV files is all of type string, even though some columns should be numeric based on the data snippet.\n",
    "\n",
    "Next: `Check Percentage of Missing Values for Each Column`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71501f",
   "metadata": {},
   "source": [
    "##### 3. Check Percentage of Missing Values for each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34c5822b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Percentage of Missing Values for each column with pyspark\n",
    "# output column_a : 100.0, column_b : 0.0, column_c : 0.0\n",
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "def missing_value(df):\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Calculate the percentage of missing values for each column\n",
    "    # use when function to check if the value is null then 1 otherwise None\n",
    "    missing = df.select([\n",
    "        (count(when(col(c).isNull(), c)) / total_count * 100).alias(c) \n",
    "        for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    return missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e5a953c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Value ---\n",
      "Data user_type\n",
      "{'user_type_id': 0.0, 'user_type_name': 0.0}\n",
      "Data station\n",
      "{'station_id': 0.0, 'station_name': 0.0, 'latitude': 0.0, 'longitude': 0.0}\n",
      "Data bike\n",
      "{'bike_id': 0.0, 'type': 0.0, 'model': 0.0, 'purchase_date': 0.0}\n",
      "Data trip_2020\n",
      "{'tripduration': 0.0, 'starttime': 0.0, 'stoptime': 0.0, 'start station id': 0.0, 'start station name': 0.0, 'start station latitude': 0.0, 'start station longitude': 0.0, 'end station id': 0.0, 'end station name': 0.0, 'end station latitude': 0.0, 'end station longitude': 0.0, 'bikeid': 0.0, 'usertype': 0.0, 'postal code': 27.873170868330526, 'year': 0.0, 'month': 0.0, 'birth year': 79.1954371360867, 'gender': 79.1954371360867}\n",
      "Data trip_2019\n",
      "{'tripduration': 0.0, 'starttime': 0.0, 'stoptime': 0.0, 'start station id': 0.0, 'start station name': 0.0, 'start station latitude': 0.0, 'start station longitude': 0.0, 'end station id': 0.0, 'end station name': 0.0, 'end station latitude': 0.0, 'end station longitude': 0.0, 'bikeid': 0.0, 'usertype': 0.0, 'birth year': 0.0, 'gender': 0.0, 'year': 0.0, 'month': 0.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Missing Value ---\")\n",
    "print(\"Data user_type\")\n",
    "print(missing_value(df_user_type))\n",
    "print(\"Data station\")\n",
    "print(missing_value(df_station))\n",
    "print(\"Data bike\")\n",
    "print(missing_value(df_bike))\n",
    "print(\"Data trip_2020\")\n",
    "print(missing_value(df_trip_2020))\n",
    "print(\"Data trip_2019\")\n",
    "print(missing_value(df_trip_2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9eece6",
   "metadata": {},
   "source": [
    "- Missing values were identified in the following columns:\n",
    "  - **Trip Data 2020**: `postal_code`, `birth_year`, and `gender`\n",
    "- For the user information, only the `user_type` column is needed. Therefore, columns related to user information, other than `user_type`, will be removed.\n",
    "\n",
    "Next: `Check Percentage of Valid Date Format`\n",
    "- Bike: column purchase_date \n",
    "- check Check Percentage of Valid Datetime Format\n",
    "- Data Trip 2020 & 2019: starttime, stoptime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5f064c",
   "metadata": {},
   "source": [
    "##### 4. Check Percentage of Valid Date "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a3f3c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check Check Percentage of Valid Date Format\n",
    "# Table Bike: column purchase_date \n",
    "# check Check Percentage of Valid Datetime Format\n",
    "# Table Data Trip 2020 & 2019: starttime, stoptime\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "def valid_date_format(df, column_name):\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Calculate the percentage of valid date format for each column\n",
    "    # if data can be converted to date then it is valid\n",
    "    valid_date = df.select([\n",
    "        (count(when(col(column_name).isNotNull() & to_date(col(column_name), 'yyyy-MM-dd').isNotNull(), column_name)) / total_count  * 100).alias(column_name) \n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    return valid_date\n",
    "\n",
    "def valid_datetime_format(df, column_name):\n",
    "    total_count = df.count()\n",
    "\n",
    "    # Calculate the percentage of valid datetime format for each column\n",
    "    # if data can be converted to datetime then it is valid\n",
    "    valid_datetime = df.select([\n",
    "        (count(when(col(column_name).isNotNull() & to_timestamp(col(column_name), 'yyyy-MM-dd HH:mm:ss.SSSS').isNotNull(), column_name)) / total_count  * 100).alias(column_name) \n",
    "    ]).collect()[0].asDict()\n",
    "\n",
    "    return valid_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1135d454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Valid Date Format ---\n",
      "Data bike\n",
      "{'purchase_date': 100.0}\n",
      "--- Valid Datetime Format ---\n",
      "Data trip_2019\n",
      "{'starttime': 100.0}\n",
      "{'stoptime': 100.0}\n",
      "Data trip_2020\n",
      "{'starttime': 100.0}\n",
      "{'stoptime': 100.0}\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Valid Date Format ---\")\n",
    "print(\"Data bike\")\n",
    "print(valid_date_format(df_bike, \"purchase_date\"))\n",
    "print(\"--- Valid Datetime Format ---\")\n",
    "print(\"Data trip_2019\")\n",
    "print(valid_datetime_format(df_trip_2019, \"starttime\"))\n",
    "print(valid_datetime_format(df_trip_2019, \"stoptime\"))\n",
    "print(\"Data trip_2020\")\n",
    "print(valid_datetime_format(df_trip_2020, \"starttime\"))\n",
    "print(valid_datetime_format(df_trip_2020, \"stoptime\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1e6c0",
   "metadata": {},
   "source": [
    "All date and datetime formats in the datasets are valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5294f1b-4973-4c01-9502-cb959198a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Profiling Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3bb696",
   "metadata": {},
   "source": [
    "### Building Data Pipeline EL Source to Staging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13298ebb",
   "metadata": {},
   "source": [
    "Solution: \n",
    "1. Pattern: EL\n",
    "    - `Data Extraction` involves retrieving data from various sources\n",
    "    - `Data Loading` involves transferring this raw data into staging systems\n",
    "2. Data Extraction:\n",
    "    - `Sources`: Extract data from CSV files and the Bluebikes database.\n",
    "    - `Techniques`: Use Full Incremental extraction for each source, as both sources lack new data markers (e.g., no `created_at` or `updated_at` columns).\n",
    "3. Data Load:\n",
    "    - `Staging`: Load raw data into a staging database (PostgreSQL) without transformation.\n",
    "    - `Techniques`: Overwrite the data in staging; data will be overwritten every time the pipeline is run.\n",
    "Untuk setiap proses akan disimpan dalam log process\n",
    "4. Data Staging Schema:\n",
    "\n",
    "<img src='https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/w7_staging_-_public.png' width=\"800\"> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09283dc-6758-483d-b791-963b715143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline Staging\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125aed62",
   "metadata": {},
   "source": [
    "#### Log System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131f04f",
   "metadata": {},
   "source": [
    "Each process in the ETL pipeline will generate log information. The log message will follow this format:\n",
    "\n",
    "``` python\n",
    "log_msg = {\n",
    "                \"step\" : \"staging | warehouse\",\n",
    "                \"process\":\"extraction | transformation | load\",\n",
    "                \"status\": \"success | failed\",\n",
    "                \"source\": \"db_bluebikes | csv | staging\",\n",
    "                \"table_name\": table_name,\n",
    "                \"etl_date\": Current timestamp\n",
    "            }\n",
    "```\n",
    "\n",
    "In utils.py, create function for load log message to database log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fc19adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import stg_engine, log_engine, wh_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def load_log(spark: SparkSession, log_msg):\n",
    "    DB_URL, DB_USER, DB_PASS = log_engine()\n",
    "    table_name = \"etl_log\"\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    log_msg.write.jdbc(url = DB_URL,\n",
    "                  table = table_name,\n",
    "                  mode = \"append\",\n",
    "                  properties = connection_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1fa392da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of log message\n",
    "from datetime import datetime\n",
    "from src.utils.helper import load_log\n",
    "\n",
    "current_timestamp = datetime.now()\n",
    "\n",
    "log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"db_bluebikes\", \"user_type\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "854f3fa8-6aac-4c6d-b796-528010736976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load log\n",
    "load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e569237",
   "metadata": {},
   "source": [
    "#### Extarct Data From Database bluebikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fcd97",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. **Create the `extract_database` Module:**\n",
    "   - Develop a module named `extract_database` within the `src/staging/extract` folder. This module will include functions to connect to the Bluebikes database and extract data from specific tables.\n",
    "\n",
    "2. **Get Database Connection URL:**\n",
    "   - Retrieve the connection URL for the Bluebikes database. This URL is necessary for connecting to the database and performing data extraction. It usually includes the hostname, port, database name, and authentication details.\n",
    "\n",
    "3. **Extract Data from Specific Table:**\n",
    "   - Implement a function within the `extract_database` module to extract data from the specific table(s) in the Bluebikes database.\n",
    "\n",
    "4. **Save Log Info to Database Log:**\n",
    "   - After extracting data, record the log information in a logging database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec429c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extarct with log\n",
    "from src.utils.helper import load_log, bluebikes_engine\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_database(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = bluebikes_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "    \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"db_bluebikes\", table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"failed\", \"db_bluebikes\", table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9faf94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.staging.extract.extract_database import extract_database\n",
    "\n",
    "# Extract data from database\n",
    "df_user_type = extract_database(spark, 'user_type')\n",
    "df_station = extract_database(spark, 'station')\n",
    "df_bike = extract_database(spark, 'bike')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cf23184-6c0d-4b00-b47d-7f37e0d60f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|user_type_id|user_type_name|\n",
      "+------------+--------------+\n",
      "|           0|    Subscriber|\n",
      "|           1|      Customer|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_user_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4c76927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+\n",
      "|station_id|        station_name|          latitude|         longitude|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "|         1|18 Dorrance Wareh...|         42.387151|        -71.075978|\n",
      "|         3|Colleges of the F...| 42.34011512249237|-71.10061883926393|\n",
      "|         4|Tremont St at E B...|         42.345392|        -71.069616|\n",
      "|         5|Northeastern Univ...|         42.341814|        -71.090179|\n",
      "|         6|Cambridge St at J...|42.361211653079856|-71.06530619789737|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_station.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a0f2847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|bike_id|    type| model|purchase_date|\n",
      "+-------+--------+------+-------------+\n",
      "|      1|    road|R88W4N|   2020-12-02|\n",
      "|      4|    road|G79YM9|   2022-10-08|\n",
      "|      7|foldable|IOEZRL|   2024-07-07|\n",
      "|      8|foldable|ADQ2DA|   2021-03-26|\n",
      "|      9|foldable|8EEQ8Z|   2023-03-04|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bike.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0853ce",
   "metadata": {},
   "source": [
    "#### Extarct Data From CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021d2376",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. **Create the `extract_csv` Module:**\n",
    "   - Develop a module named `extract_csv` within the `src/staging/extract` folder. This module will include functions to read data from CSV files.\n",
    "\n",
    "2. **Extract Data from Specific File:**\n",
    "   - Implement a function within the `extract_csv` module to read data from the specified CSV file.\n",
    "\n",
    "3. **Save Log Info to Database Log:**\n",
    "   - After extracting data, record the log information in a logging database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c07fe86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "PATH = \"data/\"\n",
    "\n",
    "def extract_csv(spark: SparkSession, file_name):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "\n",
    "        df = spark.read.csv(PATH + file_name, header=True)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"success\", \"csv\", file_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"extraction\", \"failed\", \"csv\", file_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e66dc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.staging.extract.extract_csv import extract_csv\n",
    "\n",
    "# Extract data from csv\n",
    "df_trip_2020 = extract_csv(spark, 'bluebikes_tripdata_2020.csv')\n",
    "df_trip_2019 = extract_csv(spark, 'bluebikes_tripdata_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cdb97f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|birth year|gender|year|month|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "|         790|2019-12-01 00:01:...|2019-12-01 00:14:...|             370|Dartmouth St at N...|     42.35096144421219|     -71.07782810926437|            33|      Kenmore Square|           42.348706|           -71.097009|  5133|Subscriber|      1950|     1|2019|   12|\n",
      "|         166|2019-12-01 00:05:...|2019-12-01 00:08:...|              80|MIT Stata Center ...|      42.3621312344991|     -71.09115600585936|            67|MIT at Mass Ave /...|             42.3581|           -71.093198|  2653|Subscriber|      1993|     1|2019|   12|\n",
      "|         323|2019-12-01 00:08:...|2019-12-01 00:13:...|             381|Inman Square at S...|     42.37438408515815|      -71.1001574621514|           221|Verizon Innovatio...|   42.37250864997261|   -71.11305356025694|  4875|Subscriber|      1992|     1|2019|   12|\n",
      "|         709|2019-12-01 00:08:...|2019-12-01 00:20:...|             185|     Third at Binney|    42.365444861373994|      -71.0827714204788|           184|Sidney Research C...|   42.35775309465199|   -71.10393404960631|  2116|Subscriber|      1997|     1|2019|   12|\n",
      "|         332|2019-12-01 00:10:...|2019-12-01 00:15:...|             221|Verizon Innovatio...|     42.37250864997261|     -71.11305356025694|            89|Harvard Law Schoo...|           42.379011|           -71.119945|  6156|Subscriber|      1985|     1|2019|   12|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+----------+------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_2019.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6ab9479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|tripduration|           starttime|            stoptime|start station id|  start station name|start station latitude|start station longitude|end station id|    end station name|end station latitude|end station longitude|bikeid|  usertype|postal code|year|month|birth year|gender|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "|        1793|2020-11-01 00:00:...|2020-11-01 00:30:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  4896|  Customer|      11214|2020|   11|      NULL|  NULL|\n",
      "|        1832|2020-11-01 00:00:...|2020-11-01 00:31:...|             186|Congress St at No...|               42.3481|              -71.03764|           186|Congress St at No...|             42.3481|            -71.03764|  5630|  Customer|      11220|2020|   11|      NULL|  NULL|\n",
      "|         262|2020-11-01 00:01:...|2020-11-01 00:06:...|             186|Congress St at No...|               42.3481|              -71.03764|             7|            Fan Pier|  42.353390507052296|   -71.04457139968872|  5634|Subscriber|      02128|2020|   11|      NULL|  NULL|\n",
      "|         419|2020-11-01 00:04:...|2020-11-01 00:10:...|              74|Harvard Square at...|    42.373267999999996|             -71.118579|            76|Central Sq Post O...|  42.366426000000004|   -71.10549499999999|  6071|  Customer|      02139|2020|   11|      NULL|  NULL|\n",
      "|         275|2020-11-01 00:04:...|2020-11-01 00:08:...|              73|Harvard Square at...|             42.373231|             -71.120886|           104|Harvard Universit...|           42.380287|           -71.125107|  2712|Subscriber|      21015|2020|   11|      NULL|  NULL|\n",
      "+------------+--------------------+--------------------+----------------+--------------------+----------------------+-----------------------+--------------+--------------------+--------------------+---------------------+------+----------+-----------+----+-----+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_trip_2020.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db2d806",
   "metadata": {},
   "source": [
    "#### Load Data to Satging Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a06f7c",
   "metadata": {},
   "source": [
    "*Steps:**\n",
    "\n",
    "1. **Create the `load_staging` Module:**\n",
    "   - Develop a module named `load_staging` within the `src/staging/load` folder. This module will include functions to load data into the staging area.\n",
    "\n",
    "2. **Load Data to Specific Table:**\n",
    "   - Implement a function within the `load_staging` module to load data into the specified table in the staging area. Implement `overwrite`\n",
    "\n",
    "3. **Save Log Info to Database Log:**\n",
    "   - After loading data, record the log information in a logging database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1b7dc6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_log, stg_engine  \n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def load_staging(spark: SparkSession, df, table_name, source_name):\n",
    "    current_timestamp = datetime.now()\n",
    "    DB_URL, DB_USER, DB_PASS = stg_engine()\n",
    "    properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS\n",
    "    }\n",
    "    try:\n",
    "        df.write.jdbc(url = DB_URL,\n",
    "                    table = table_name,\n",
    "                    mode = \"overwrite\",\n",
    "                    properties = properties)\n",
    "        \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"staging\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "    finally:\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b85b854-00f5-43ac-90ca-d84206675d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to staging\n",
    "from src.staging.load.load_staging import load_staging\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "# add column created_at \n",
    "df_user_type = df_user_type.withColumn(\"created_at\", current_timestamp())\n",
    "df_station = df_station.withColumn(\"created_at\", current_timestamp())\n",
    "df_bike = df_bike.withColumn(\"created_at\", current_timestamp())\n",
    "df_trip_2019 = df_trip_2019.withColumn(\"created_at\", current_timestamp())\n",
    "df_trip_2020 = df_trip_2020.withColumn(\"created_at\", current_timestamp())\n",
    "\n",
    "\n",
    "load_staging(spark, df_user_type, \"user_type\", \"db_bluebikes\")\n",
    "load_staging(spark, df_station, \"station\", \"db_bluebikes\")\n",
    "load_staging(spark, df_bike, \"bike\", \"db_bluebikes\")\n",
    "load_staging(spark, df_trip_2019, \"trip_data_2019\", \"csv\")\n",
    "load_staging(spark, df_trip_2020, \"trip_data_2020\", \"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d26862f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Profiling Session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbabfbe",
   "metadata": {},
   "source": [
    "`Create Views for Combined and Filtered Data`\n",
    "To simplify the transformation process and meet business requirements, we will create views that combine data from multiple files and apply necessary filters. This approach helps streamline the data transformation steps by consolidating relevant data and applying filters at the view level.\n",
    "- usertype = \"Subscriber\"\n",
    "- tripduration > 500 seconds\n",
    "\n",
    "``` sql\n",
    "CREATE VIEW combined_trip_data AS\n",
    "SELECT \n",
    "    tripduration,\n",
    "    starttime,\n",
    "    stoptime,\n",
    "    \"start station id\",\n",
    "    \"start station name\",\n",
    "    \"start station latitude\",\n",
    "    \"start station longitude\",\n",
    "    \"end station id\",\n",
    "    \"end station name\",\n",
    "    \"end station latitude\",\n",
    "    \"end station longitude\",\n",
    "    bikeid,\n",
    "    usertype,\n",
    "    \"birth year\",\n",
    "    gender,\n",
    "    NULL AS \"postal code\",  -- Adding NULL for missing column\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    created_at\n",
    "FROM trip_data_2019\n",
    "WHERE usertype = 'Subscriber' AND tripduration::int > 500\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT \n",
    "    tripduration,\n",
    "    starttime,\n",
    "    stoptime,\n",
    "    \"start station id\",\n",
    "    \"start station name\",\n",
    "    \"start station latitude\",\n",
    "    \"start station longitude\",\n",
    "    \"end station id\",\n",
    "    \"end station name\",\n",
    "    \"end station latitude\",\n",
    "    \"end station longitude\",\n",
    "    bikeid,\n",
    "    usertype,\n",
    "    \"birth year\",\n",
    "    gender,\n",
    "    \"postal code\",  -- Including the actual column\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    created_at\n",
    "FROM trip_data_2020\n",
    "WHERE usertype = 'Subscriber' AND tripduration::int > 500;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e72866",
   "metadata": {},
   "source": [
    "### Building Data Pipeline EL Staging to Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8dd322",
   "metadata": {},
   "source": [
    "Solution: \n",
    "1. Pattern: ETL\n",
    "    - `Data Extraction` involves retrieving data from staging area\n",
    "    - `Data Transformation` involve transformed data to fit the desired format or data warehouse structure.\n",
    "    - `Data Loading` involves transferring this staging data into data warehouse\n",
    "2. Data Extraction:\n",
    "    - Sources: Extract data from staging area.\n",
    "    - Techniques: Full Ingestion is used to extract the complete dataset from the staging area.\n",
    "3. Data Load:\n",
    "    - Data Warehouse: Load clean, transformed and valid data to the final destination.\n",
    "    - Techniques: Use the `TRUNCATE TABLE` statement to clear existing data, followed by `append` operations to load the new data.\n",
    "\n",
    "4. Data Transformation:\n",
    "    - Transformation: Adjust data to fit the desired format or structure of the data warehouse.\n",
    "    - Techniques: Joining, Filtering, Aggregation, Deduplication, Conversion, Structuring, etc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3b2dc7",
   "metadata": {},
   "source": [
    "##### Target Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a704d",
   "metadata": {},
   "source": [
    "\n",
    "In the data warehouse, the target schema will be designed using a `dimensional model`\n",
    "Dimension Table:\n",
    "- dim_date\n",
    "- dim_time\n",
    "- dim_bike\n",
    "- dim_station\n",
    "- dim_user_type\n",
    "\n",
    "Fact Table\n",
    "- fact_trip_data\n",
    "- fact_bike_usage\n",
    "\n",
    "\n",
    "<img src= 'https://sekolahdata-assets.s3.ap-southeast-1.amazonaws.com/notebook-images/mde-data-ingestion-spark/w7_warehouse_-_public.png' width=\"800\"> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d939a76",
   "metadata": {},
   "source": [
    "##### Source to Target Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50917840",
   "metadata": {},
   "source": [
    "\n",
    "Source to Target Mapping Documentation: [Link](https://github.com/Kurikulum-Sekolah-Pacmann/pipeline-clinic/blob/main/target_mapping_warehouse.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01bca7",
   "metadata": {},
   "source": [
    "##### Validation Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acd9436",
   "metadata": {},
   "source": [
    "Validation Rule:\n",
    "- Trip data that will enter the warehouse is data with the user \"Subscriber\" and \"trip duration\" which is more than 500 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e528c78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# # create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pipeline Warehouse\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4fcf26",
   "metadata": {},
   "source": [
    "##### Extract Data From Staging Area"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7730e9d",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. **Create the `extract_staging` Module:**\n",
    "   - Develop a module named `extract_staging` within the `src/warehouse/extract` folder. This module will include functions to connect to the staging database and extract data from specific tables.\n",
    "\n",
    "2. **Get Database Connection URL:**\n",
    "   - Retrieve the connection URL for the staging database. This URL is necessary for connecting to the staging database and performing data extraction\n",
    "\n",
    "3. **Extract Data from Specific Table:**\n",
    "   - Implement a function within the `extract_staging` module to extract data from the specific table(s) in the staging database.\n",
    "\n",
    "4. **Save Log Info to Database Log:**\n",
    "   - After extracting data, record the log information in a logging database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af514ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extarct with log\n",
    "from src.utils.helper import load_log, stg_engine\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_staging(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = stg_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "    \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"extraction\", \"success\", \"staging\", table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"extraction\", \"failed\", \"staging\", table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b21332bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from staging\n",
    "from src.warehouse.extract.extract_staging import extract_staging\n",
    "\n",
    "# Extarc Data fro Dmension Table\n",
    "df_user_type = extract_staging(spark, 'user_type')\n",
    "df_station = extract_staging(spark, 'station')\n",
    "df_bike = extract_staging(spark, 'bike')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85f4887",
   "metadata": {},
   "source": [
    "##### Load Data to Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a033c475",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. **Create the `load_warehouse` Module:**\n",
    "   - Develop a module named `load_warehouse` within the `src/warehouse/load` folder. This module will include functions for connecting to the data warehouse, truncating tables, and loading data.\n",
    "\n",
    "2. **Get Database Connection URL:**\n",
    "   - Retrieve the connection URL for the data warehouse. \n",
    "\n",
    "3. **Truncate Target Table:**\n",
    "   - Implement functionality within the `load_warehouse` module to truncate the target table in the data warehouse. Use the `TRUNCATE TABLE` SQL statement to clear existing data from the table.\n",
    "\n",
    "4. **Load Data to Data Warehouse:**\n",
    "   - Use the `Append` operation from pyspark to load the transformed data into the target table in the data warehouse\n",
    "\n",
    "5. **Save Log Info to Database Log:**\n",
    "   - After loading the data, record the log information in a logging database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ee4c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.helper import load_log, wh_engine, wh_engine_sqlalchemy\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "\n",
    "\n",
    "# before load to warehouse, truncate the table with sqlalchemy\n",
    "def load_warehouse(spark: SparkSession, df, table_name, source_name):\n",
    "    current_timestamp = datetime.now()\n",
    "    DB_URL, DB_USER, DB_PASS = wh_engine()\n",
    "    properties = {\n",
    "    \"user\": DB_USER,\n",
    "    \"password\": DB_PASS\n",
    "    }\n",
    "    try:\n",
    "        # truncate table with sqlalchemy\n",
    "        conn = wh_engine_sqlalchemy()\n",
    "\n",
    "        with conn.connect() as connection:\n",
    "            # Execute the TRUNCATE TABLE command\n",
    "            connection.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE \"))\n",
    "            connection.commit()\n",
    "            connection.close()\n",
    "        conn.dispose()\n",
    "    except Exception as e:\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        load_log(spark, log_msg)\n",
    "    \n",
    "    try:\n",
    "        # load data\n",
    "        df.write.jdbc(url = DB_URL,\n",
    "                    table = table_name,\n",
    "                    mode = \"append\",\n",
    "                    properties = properties)\n",
    "        \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"success\", source_name, table_name, current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(e)\n",
    "        \n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"load\", \"failed\", source_name, table_name, current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "        \n",
    "    finally:\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abcf601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.load.load_warehouse import load_warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358c61c3",
   "metadata": {},
   "source": [
    "##### Transform Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18484e4",
   "metadata": {},
   "source": [
    "**Steps:**\n",
    "\n",
    "1. **Create the `table_name.py` Module:**\n",
    "   - Develop a module named `table_name.py` within the `src/warehouse/transform` folder. This module will include functions to handle transformations for specific tables in the data warehouse.\n",
    "\n",
    "2. **Develop Transformation Modules:**\n",
    "   - Create transformation functions for the tables that require data transformation. Implement these functions to perform necessary operations such as joining, filtering, aggregating, or converting data to fit the warehouse schema.\n",
    "\n",
    "3. **Perform Data Transformation:**\n",
    "   - Use the transformation functions to process data from specific tables. Apply the transformations to ensure that the data adheres to the desired format and structure of the data warehouse.\n",
    "\n",
    "4. **Save Log Info to Database Log:**\n",
    "   - After performing data transformations, record the log information in a logging database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42184f39",
   "metadata": {},
   "source": [
    "Transform Data user_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d46130",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Table: `user_type`      | Target Table: `dim_user_type`   | Description                    |\n",
    "|--------------------------------|---------------------------------|--------------------------------|\n",
    "| `user_type_id` (int8)          | `user_type_nk` (int4)           | Rename                         |\n",
    "| -                              | `user_type_id` (UUID)           | Default Value                     |\n",
    "| `user_type_name` (text)        | `user_type_name` (varchar)      | Direct Mapping                 |\n",
    "| -                              | `created_at` (timestamp)        | Default Value       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ce2cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_user_type(spark, df):\n",
    "\n",
    "    current_timestamp = datetime.now()\n",
    "    try:\n",
    "        # rename column user_type_id to user_type_nk\n",
    "        df = df.withColumnRenamed(\"user_type_id\", \"user_type_nk\")\n",
    "\n",
    "        # drop column created_at\n",
    "        df = df.drop(\"created_at\")\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"user_type\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"user_type\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "    finally:\n",
    "        # load log\n",
    "        print(log_msg.show())\n",
    "        # load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e9e24f78-7c3d-470c-9b57-3bda4171c07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.user_type import transform_user_type\n",
    "\n",
    "user_type_transformed = transform_user_type(spark, df_user_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83ed784e-b040-43ea-8a9b-844ace0f6075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------+\n",
      "|user_type_nk|user_type_name|\n",
      "+------------+--------------+\n",
      "|           0|    Subscriber|\n",
      "|           1|      Customer|\n",
      "+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_type_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e32c1bf4-7053-4aaf-b346-6f1c8087486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, user_type_transformed, \"dim_user_type\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a0c2fe",
   "metadata": {},
   "source": [
    "Transform Data Bike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb4013",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Table: `bike`          | Target Table: `dim_bike`       | Description                    |\n",
    "|-------------------------------|--------------------------------|--------------------------------|\n",
    "| `bike_id` (int8)              | `bike_nk` (int4)               | Rename                         |\n",
    "|   -                            | `bike_id` (UUID)               | default value                   |\n",
    "| `type` (text)                 | `type` (varchar)               | Direct Mapping                 |\n",
    "| `model` (text)                | `model` (varchar)              | Direct Mapping                 |\n",
    "| `purchase_date` (date)        | `purchase_date` (date)         | Direct Mapping                 |\n",
    "| -    | `created_at` (timestamp)       |  default value  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b04a9cb5-b5a2-432f-a5cc-adf398e68ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_bike(spark, df):\n",
    "    current_timestamp = datetime.now()\n",
    "    try:\n",
    "        # rename column bike_id to bike_nk\n",
    "        df = df.withColumnRenamed(\"bike_id\", \"bike_nk\")\n",
    "\n",
    "        # drop column created_at\n",
    "        df = df.drop(\"created_at\")\n",
    "\n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"bike\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"bike\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "819c4bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform data bike\n",
    "from src.warehouse.transformation.bike import transform_bike\n",
    "\n",
    "bike_transformed = transform_bike(spark, df_bike)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "24434bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+------+-------------+\n",
      "|bike_nk|    type| model|purchase_date|\n",
      "+-------+--------+------+-------------+\n",
      "|      1|    road|R88W4N|   2020-12-02|\n",
      "|      4|    road|G79YM9|   2022-10-08|\n",
      "|      7|foldable|IOEZRL|   2024-07-07|\n",
      "|      8|foldable|ADQ2DA|   2021-03-26|\n",
      "|      9|foldable|8EEQ8Z|   2023-03-04|\n",
      "+-------+--------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bike_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d67a049-dfa0-4dc4-818f-3ac095df51b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, bike_transformed, \"dim_bike\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c79a84",
   "metadata": {},
   "source": [
    "Transform Data Station"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52466d1",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Table: `station`   | Target Table: `dim_station`   |Description                 |\n",
    "|---------------------------|-------------------------------|-------------------------------|\n",
    "| `station_id` (int8)       | `station_nk` (int4)           | Rename from `station_id` to `station_nk` |\n",
    "|                           | `station_id` (UUID)           |Default Value |\n",
    "| `station_name` (text)     | `station_name` (varchar)      | Direct Mapping                   |\n",
    "| `latitude` (float8)       | `latitude` (numeric)          | Direct Mapping |\n",
    "| `longitude` (float8)      | `longitude` (numeric)         | Direct Mapping |\n",
    "| `created_at` (timestamp)  | `created_at` (timestamp)      | Default Value |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "57a811b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_station(spark, df):\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # rename column station_id to station_nk\n",
    "        df = df.withColumnRenamed(\"station_id\", \"station_nk\")\n",
    "\n",
    "        # drop column created_at\n",
    "        df = df.drop(\"created_at\")\n",
    "      \n",
    "        #log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"station\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"station\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7943bb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.station import transform_station\n",
    "\n",
    "station_transformed = transform_station(spark, df_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6b891553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------------+------------------+\n",
      "|station_nk|        station_name|          latitude|         longitude|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "|         1|18 Dorrance Wareh...|         42.387151|        -71.075978|\n",
      "|         3|Colleges of the F...| 42.34011512249237|-71.10061883926393|\n",
      "|         4|Tremont St at E B...|         42.345392|        -71.069616|\n",
      "|         5|Northeastern Univ...|         42.341814|        -71.090179|\n",
      "|         6|Cambridge St at J...|42.361211653079856|-71.06530619789737|\n",
      "+----------+--------------------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "station_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d94ac2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, station_transformed, \"dim_station\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6620e963",
   "metadata": {},
   "source": [
    "Extarct Data for Fact Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4243cab",
   "metadata": {},
   "source": [
    "Extract data from view : `combined_trip_data`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "71c9fdfd-0077-4562-9349-6d98cbfdb777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extarct data for Fact Table\n",
    "df_trip = extract_staging(spark, 'combined_trip_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e25eb",
   "metadata": {},
   "source": [
    "Transfrom data fact_trip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccd97f6",
   "metadata": {},
   "source": [
    "Source to Target Mapping\n",
    "\n",
    "| Source Tables: `trip_data_2019`, `trip_data_2020` | Target Table: `fact_trip_data` | **Description**                              |\n",
    "|--------------------------------------------------|-------------------------------|-------------------------------                 |\n",
    "| `tripduration` (text)                             | `trip_duration` (int4)        | Data Type Conversion                          |\n",
    "| `starttime` (text)                                | `start_date` (int4)           | Convert Date Part Lookup to `dim_date`        |\n",
    "| `starttime` (text)                                | `start_time` (int4)           | Convert Time Part Lookup to `dim_time`        |\n",
    "| `stoptime` (text)                                 | `stop_date` (int4)            | Convert Date Part Lookup to `dim_date`        |\n",
    "| `stoptime` (text)                                 | `stop_time` (int4)            | Convert Time Part Lookup to `dim_time`        |\n",
    "| `\"start station id\"` (text)                       | `start_station_id` (uuid)     | Lookup to `dim_station`, Data Type Conversion |\n",
    "| `\"end station id\"` (text)                         | `end_station_id` (uuid)       | Lookup to `dim_station`, Data Type Conversion |\n",
    "| `\"start station name\"` (text)                     | -                             | Not Mapped |\n",
    "| `\"start station latitude\"` (text)                 | -                             | Not Mapped |\n",
    "| `\"start station longitude\"` (text)                | -                             | Not Mapped |\n",
    "| `\"end station name\"` (text)                       | -                             | Not Mapped |\n",
    "| `\"end station latitude\"` (text)                   | -                             | Not Mapped |\n",
    "| `\"end station longitude\"` (text)                  | -                             | Not Mapped |\n",
    "| `bikeid` (text)                                   | `bike_id` (uuid)              | Lookup to `dim_bike`, Data Type Conversion    |\n",
    "| `usertype` (text)                                 | `user_type_id` (uuid)         | Lookup to `dim_user_type`, Data Type Conversion |\n",
    "| `year` (text)                                     | `year` (varchar)              | Direct Mapping                                |\n",
    "| `month` (text)                                    | `month` (varchar)             | Direct Mapping                                |\n",
    "| `postal code` (text)                              | -                             | Not Mapped                                    |\n",
    "| `birth year` (text)                               | -                             | Not Mapped                                    |\n",
    "| `gender` (text)                                   | -                             | Not Mapped                                    |\n",
    "| `created_at` (timestamp)                          | -                             | Default Value                                 |\n",
    "| -                                                 | `trip_id` (uuid)              | Generated UUID                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad25fb30",
   "metadata": {},
   "source": [
    "`Create Module to Extract Data from Dimension Tables`\n",
    "\n",
    "Use the `extract_warehouse.py` functions to extract data from dimension tables. Ensure that the data extracted is accurate and complete, as it will be used to establish foreign key relationships with fact tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96cd0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Extarct with log\n",
    "from src.utils.helper import wh_engine\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_warehouse(spark: SparkSession, table_name):\n",
    "    # get config\n",
    "    DB_URL, DB_USER, DB_PASS = wh_engine()\n",
    "\n",
    "    # set config\n",
    "    connection_properties = {\n",
    "        \"user\": DB_USER,\n",
    "        \"password\": DB_PASS,\n",
    "        \"driver\": \"org.postgresql.Driver\" # set driver postgres\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # read data\n",
    "        df = spark \\\n",
    "                .read \\\n",
    "                .jdbc(url = DB_URL,\n",
    "                        table = table_name,\n",
    "                        properties = connection_properties)\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4db88ee7-8e24-4eef-ba1e-ee75ca3e4d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import date_format, col\n",
    "from src.warehouse.extract.extract_warehouse import extract_warehouse\n",
    "\n",
    "def transform_fact_trip(spark: SparkSession, df):\n",
    "    current_timestamp = datetime.now()\n",
    "\n",
    "    try:\n",
    "        # Extarct data dimension table\n",
    "        df_user_type = extract_warehouse(spark, \"dim_user_type\", [\"user_type_id\", \"user_type_name\"])\n",
    "        df_station = extract_warehouse(spark, \"dim_station\", [\"station_id\", \"station_nk\"])\n",
    "        df_bike = extract_warehouse(spark, \"dim_bike\", [\"bike_id\", \"bike_nk\"])\n",
    "        df_date = extract_warehouse(spark, \"dim_date\", [\"date_id\", \"date_actual\"])\n",
    "        df_time = extract_warehouse(spark, \"dim_time\", [\"time_id\", \"time_actual\"])\n",
    "\n",
    "        # convert df_time column time_actual to HH:mm:ss\n",
    "        df_time = df_time.withColumn(\"time_actual\", date_format(col(\"time_actual\"),  \"HH:mm:ss\"))\n",
    "\n",
    "        #convert tripduration to int \n",
    "        df = df.withColumn(\"tripduration\", df[\"tripduration\"].cast(\"int\"))\n",
    "\n",
    "        # rename column\n",
    "        df = df.withColumnRenamed(\"tripduration\", \"trip_duration\") \\\n",
    "                .withColumnRenamed(\"start station id\", \"start_station_nk\") \\\n",
    "                .withColumnRenamed(\"end station id\", \"end_station_nk\") \\\n",
    "                .withColumnRenamed(\"bikeid\", \"bike_nk\") \\\n",
    "                .withColumnRenamed(\"usertype\", \"user_type_name\")\n",
    "    \n",
    "        \n",
    "        # extract date and time from starttime and stoptime\n",
    "        df = df.withColumn(\"start_date_temp\", date_format(col(\"starttime\"),  \"yyyy-MM-dd\")) \\\n",
    "                .withColumn(\"start_time_temp\", date_format(col(\"starttime\"),  \"HH:mm:00\")) \\\n",
    "                .withColumn(\"stop_date_temp\", date_format(col(\"stoptime\"),  \"yyyy-MM-dd\")) \\\n",
    "                .withColumn(\"stop_time_temp\", date_format(col(\"stoptime\"),  \"HH:mm:00\"))\n",
    "        \n",
    "        # get bike_id from dim_bike\n",
    "        df = df.join(df_bike, df.bike_nk == df_bike.bike_nk, \"left\") \\\n",
    "                .drop(df_bike.bike_nk)\n",
    "        \n",
    "        # get user_type_id from dim_user_type\n",
    "        df = df.join(df_user_type, df.user_type_name == df_user_type.user_type_name, \"left\") \\\n",
    "                .drop(df_user_type.user_type_name)\n",
    "        \n",
    "        # get start_station_id from dim_station\n",
    "        df = df.join(df_station, df.start_station_nk == df_station.station_nk, \"left\") \\\n",
    "                .drop(df_station.station_nk)\n",
    "        # rename column station_id to start_station_id\n",
    "        df = df.withColumnRenamed(\"station_id\", \"start_station_id\")\n",
    "\n",
    "        # get end_station_id from dim_station\n",
    "        df = df.join(df_station, df.end_station_nk == df_station.station_nk, \"left\") \\\n",
    "                .drop(df_station.station_nk)\n",
    "        # rename column station_id to end_station_id\n",
    "        df = df.withColumnRenamed(\"station_id\", \"end_station_id\")\n",
    "\n",
    "        # get date_id from dim_date\n",
    "        df = df.join(df_date, df.start_date_temp == df_date.date_actual, \"left\") \\\n",
    "                .drop(df_date.date_actual)\n",
    "        # rename column date_id to start_date\n",
    "        df = df.withColumnRenamed(\"date_id\", \"start_date\")\n",
    "\n",
    "        # get date_id from dim_date\n",
    "        df = df.join(df_date, df.stop_date_temp == df_date.date_actual, \"left\") \\\n",
    "                .drop(df_date.date_actual)\n",
    "        # rename column date_id to stop_date\n",
    "        df = df.withColumnRenamed(\"date_id\", \"stop_date\")\n",
    "\n",
    "        # get time_id from dim_time\n",
    "        df = df.join(df_time, df.start_time_temp == df_time.time_actual, \"left\") \\\n",
    "                .drop(df_time.time_actual)\n",
    "        # rename column time_id to start_time\n",
    "        df = df.withColumnRenamed(\"time_id\", \"start_time\")\n",
    "\n",
    "        # get time_id from dim_time\n",
    "        df = df.join(df_time, df.stop_time_temp == df_time.time_actual, \"left\") \\\n",
    "                .drop(df_time.time_actual)\n",
    "        # rename column time_id to stop_time\n",
    "        df = df.withColumnRenamed(\"time_id\", \"stop_time\")\n",
    "\n",
    "        \n",
    "        # drop unnecessary column created_at, postal code, birth year, start station name, end station name, \n",
    "        # start station latitude, start station longitude, end station latitude, end station longitude,\n",
    "        # bike_nk, user_type_name, start_station_nk, end_station_nk\n",
    "        if \"postal code\" in df.columns:\n",
    "            df = df.drop(\"postal code\")\n",
    "\n",
    "        df = df.drop(\"created_at\", \"birth year\", \"start station name\", \"end station name\", \"gender\",\n",
    "                    \"start station latitude\", \"start station longitude\", \"end station latitude\", \"end station longitude\",\n",
    "                    \"bike_nk\", \"user_type_name\", \"start_station_nk\", \"end_station_nk\", \"starttime\",\"stoptime\", \"station_nk\",\n",
    "                    \"start_date_temp\",\"stop_date_temp\", \"start_time_temp\",\"stop_time_temp\", \"date_actual\", \"time_actual\")\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"trip_data\", current_timestamp)])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "        # log message\n",
    "        log_msg = spark.sparkContext\\\n",
    "            .parallelize([(\"warehouse\", \"transform\", \"failed\", \"staging\", \"trip_data\", current_timestamp, str(e))])\\\n",
    "            .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])\n",
    "    finally:\n",
    "        # load log\n",
    "        load_log(spark, log_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fee966f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.fact_trip import transform_fact_trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ea1c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_transformed = transform_fact_trip(spark, df_trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a1a5830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trip_duration', 'int'),\n",
       " ('year', 'string'),\n",
       " ('month', 'string'),\n",
       " ('bike_id', 'bigint'),\n",
       " ('user_type_id', 'bigint'),\n",
       " ('start_station_id', 'bigint'),\n",
       " ('end_station_id', 'bigint'),\n",
       " ('start_date', 'int'),\n",
       " ('stop_date', 'int'),\n",
       " ('start_time', 'int'),\n",
       " ('stop_time', 'int')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show column name\n",
    "trip_transformed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6eaf0579-be6e-4c88-9ceb-9f1539e97d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, trip_transformed, \"fact_trip_data\", 'staging')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041207c6",
   "metadata": {},
   "source": [
    "Transform data fact_bike_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c296b8",
   "metadata": {},
   "source": [
    "`Create fact_bike_usage from Aggregated `fact_trip_data`\n",
    "\n",
    "**Steps:**\n",
    "1. **Extract Data from `fact_trip_data`:**\n",
    "   - Use the `extract_staging` module to retrieve data from the `fact_trip_data` table in the staging area. This data will be used as the basis for aggregation.\n",
    "\n",
    "2. **Group by `bike_id`:**\n",
    "   - Apply a `GROUP BY` operation on the `bike_id` column to aggregate the data based on each bike.\n",
    "\n",
    "3. **Count `trip_id`:**\n",
    "   - For each `bike_id`, count the number of trips (`trip_id`) to determine the total number of trips made by each bike.\n",
    "\n",
    "4. **Sum `trip_duration`:**\n",
    "   - Calculate the total trip duration by summing the `trip_duration` for each `bike_id`. This will provide the total time spent on trips for each bike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fb299ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from src.utils.helper import load_log\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import date_format, col\n",
    "from src.warehouse.extract.extract_warehouse import extract_warehouse\n",
    "\n",
    "def transform_fact_bike_usage(spark: SparkSession):\n",
    "        current_timestamp = datetime.now()\n",
    "\n",
    "        try:\n",
    "                # Extarct data fact_trip_data\n",
    "                columns = ['trip_duration', 'trip_id', 'bike_id']\n",
    "\n",
    "                df_trip = extract_warehouse(spark, \"fact_trip_data\", columns)\n",
    "\n",
    "                # group by bike_id and count trip_id, sum trip_duration\n",
    "                df_bike_usage = df_trip.groupBy(\"bike_id\") \\\n",
    "                                .agg({\"trip_duration\": \"sum\", \"trip_id\": \"count\"}) \\\n",
    "                                .withColumnRenamed(\"sum(trip_duration)\", \"total_duration\") \\\n",
    "                                .withColumnRenamed(\"count(trip_id)\", \"trip_count\")\n",
    "                \n",
    "                #log message\n",
    "                log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fact_bike_usage\", current_timestamp)])\\\n",
    "                .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date'])\n",
    "\n",
    "                return df_bike_usage\n",
    "        except Exception as e:\n",
    "                #log message\n",
    "                print(e)\n",
    "                log_msg = spark.sparkContext\\\n",
    "                .parallelize([(\"warehouse\", \"transform\", \"success\", \"staging\", \"fact_bike_usage\", current_timestamp, str(e))])\\\n",
    "                .toDF(['step', 'process', 'status', 'source', 'table_name', 'etl_date', 'error_msg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bf9f1560-023c-4d9c-a57c-609f5a22c2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warehouse.transformation.fact_bike_usage import transform_fact_bike_usage\n",
    "fact_bike_usage = transform_fact_bike_usage(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ede8f90-865f-4d80-be0a-0bb132af3045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bike_id', 'trip_count', 'total_duration']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_bike_usage.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ba4a8a7-70a4-4f31-afee-3619fb8dfae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data to warehouse\n",
    "load_warehouse(spark, fact_bike_usage, \"fact_bike_usage\", 'staging')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
